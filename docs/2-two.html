<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Theoretical background | thesis.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Theoretical background | thesis.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Theoretical background | thesis.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-introduction.html"/>
<link rel="next" href="3-three.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><i class="fa fa-check"></i>Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</a>
<ul>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#keywords"><i class="fa fa-check"></i>Keywords</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#abbreviations"><i class="fa fa-check"></i>Abbreviations</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-introduction.html"><a href="1-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-introduction.html"><a href="1-introduction.html#context"><i class="fa fa-check"></i><b>1.1</b> Context</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction.html"><a href="1-introduction.html#objective"><i class="fa fa-check"></i><b>1.2</b> Objective</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction.html"><a href="1-introduction.html#onethree"><i class="fa fa-check"></i><b>1.3</b> Related work</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-introduction.html"><a href="1-introduction.html#onethreeone"><i class="fa fa-check"></i><b>1.3.1</b> Forecasting in station-based systems</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-introduction.html"><a href="1-introduction.html#onethreetwo"><i class="fa fa-check"></i><b>1.3.2</b> Forecasting in dockless systems</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-introduction.html"><a href="1-introduction.html#approach"><i class="fa fa-check"></i><b>1.4</b> Approach</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction.html"><a href="1-introduction.html#outline"><i class="fa fa-check"></i><b>1.5</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-two.html"><a href="2-two.html"><i class="fa fa-check"></i><b>2</b> Theoretical background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-two.html"><a href="2-two.html#time-series-definition"><i class="fa fa-check"></i><b>2.1</b> Time series definition</a></li>
<li class="chapter" data-level="2.2" data-path="2-two.html"><a href="2-two.html#time-series-characteristics"><i class="fa fa-check"></i><b>2.2</b> Time series characteristics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-two.html"><a href="2-two.html#twotwoone"><i class="fa fa-check"></i><b>2.2.1</b> Autocorrelation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-two.html"><a href="2-two.html#stationarity"><i class="fa fa-check"></i><b>2.2.2</b> Stationarity</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-two.html"><a href="2-two.html#twotwothree"><i class="fa fa-check"></i><b>2.2.3</b> Spectral entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-two.html"><a href="2-two.html#time-series-components"><i class="fa fa-check"></i><b>2.3</b> Time series components</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-two.html"><a href="2-two.html#definitions"><i class="fa fa-check"></i><b>2.3.1</b> Definitions</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-two.html"><a href="2-two.html#classical-decomposition"><i class="fa fa-check"></i><b>2.3.2</b> Classical decomposition</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-two.html"><a href="2-two.html#twothreethree"><i class="fa fa-check"></i><b>2.3.3</b> STL decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-two.html"><a href="2-two.html#twofour"><i class="fa fa-check"></i><b>2.4</b> Time series forecasting</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-two.html"><a href="2-two.html#twofourone"><i class="fa fa-check"></i><b>2.4.1</b> Forecasting models</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-two.html"><a href="2-two.html#arima"><i class="fa fa-check"></i><b>2.4.2</b> ARIMA</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-two.html"><a href="2-two.html#twofourthree"><i class="fa fa-check"></i><b>2.4.3</b> Naïve forecasts</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-two.html"><a href="2-two.html#twofourfour"><i class="fa fa-check"></i><b>2.4.4</b> Seasonal forecasts</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-two.html"><a href="2-two.html#time-series-clustering"><i class="fa fa-check"></i><b>2.5</b> Time series clustering</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="2-two.html"><a href="2-two.html#dissimilarity-measures"><i class="fa fa-check"></i><b>2.5.1</b> Dissimilarity measures</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-two.html"><a href="2-two.html#hierarchical-clustering"><i class="fa fa-check"></i><b>2.5.2</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-two.html"><a href="2-two.html#twofivethree"><i class="fa fa-check"></i><b>2.5.3</b> Spatial time series clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-three.html"><a href="3-three.html"><i class="fa fa-check"></i><b>3</b> System architecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-three.html"><a href="3-three.html#overall-design"><i class="fa fa-check"></i><b>3.1</b> Overall design</a></li>
<li class="chapter" data-level="3.2" data-path="3-three.html"><a href="3-three.html#threetwo"><i class="fa fa-check"></i><b>3.2</b> Software</a></li>
<li class="chapter" data-level="3.3" data-path="3-three.html"><a href="3-three.html#threethree"><i class="fa fa-check"></i><b>3.3</b> System area</a></li>
<li class="chapter" data-level="3.4" data-path="3-three.html"><a href="3-three.html#threefour"><i class="fa fa-check"></i><b>3.4</b> Database</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-three.html"><a href="3-three.html#threefourone"><i class="fa fa-check"></i><b>3.4.1</b> Distance data</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-three.html"><a href="3-three.html#threefourtwo"><i class="fa fa-check"></i><b>3.4.2</b> Usage data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-three.html"><a href="3-three.html#threefive"><i class="fa fa-check"></i><b>3.5</b> Forecast request</a></li>
<li class="chapter" data-level="3.6" data-path="3-three.html"><a href="3-three.html#threesix"><i class="fa fa-check"></i><b>3.6</b> Cluster loop</a></li>
<li class="chapter" data-level="3.7" data-path="3-three.html"><a href="3-three.html#threeseven"><i class="fa fa-check"></i><b>3.7</b> Model loop</a></li>
<li class="chapter" data-level="3.8" data-path="3-three.html"><a href="3-three.html#threeeight"><i class="fa fa-check"></i><b>3.8</b> Forecast loop</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-four.html"><a href="4-four.html"><i class="fa fa-check"></i><b>4</b> Data and experimental design</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-four.html"><a href="4-four.html#fourone"><i class="fa fa-check"></i><b>4.1</b> Data source</a></li>
<li class="chapter" data-level="4.2" data-path="4-four.html"><a href="4-four.html#data-retrieval"><i class="fa fa-check"></i><b>4.2</b> Data retrieval</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-four.html"><a href="4-four.html#distance-data"><i class="fa fa-check"></i><b>4.2.1</b> Distance data</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-four.html"><a href="4-four.html#usage-data"><i class="fa fa-check"></i><b>4.2.2</b> Usage data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-four.html"><a href="4-four.html#experimental-design"><i class="fa fa-check"></i><b>4.3</b> Experimental design</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-four.html"><a href="4-four.html#training-and-test-periods"><i class="fa fa-check"></i><b>4.3.1</b> Training and test periods</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-four.html"><a href="4-four.html#additional-software"><i class="fa fa-check"></i><b>4.3.2</b> Additional software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-five.html"><a href="5-five.html"><i class="fa fa-check"></i><b>5</b> Results and discussion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-five.html"><a href="5-five.html#clustering"><i class="fa fa-check"></i><b>5.1</b> Clustering</a></li>
<li class="chapter" data-level="5.2" data-path="5-five.html"><a href="5-five.html#model-building"><i class="fa fa-check"></i><b>5.2</b> Model building</a></li>
<li class="chapter" data-level="5.3" data-path="5-five.html"><a href="5-five.html#forecasting-1"><i class="fa fa-check"></i><b>5.3</b> Forecasting</a></li>
<li class="chapter" data-level="5.4" data-path="5-five.html"><a href="5-five.html#limitations-and-recommendations"><i class="fa fa-check"></i><b>5.4</b> Limitations and recommendations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="5-five.html"><a href="5-five.html#limits-of-forecastability"><i class="fa fa-check"></i><b>5.4.1</b> Limits of forecastability</a></li>
<li class="chapter" data-level="5.4.2" data-path="5-five.html"><a href="5-five.html#exogenous-variables"><i class="fa fa-check"></i><b>5.4.2</b> Exogenous variables</a></li>
<li class="chapter" data-level="5.4.3" data-path="5-five.html"><a href="5-five.html#residual-distributions-and-prediction-intervals"><i class="fa fa-check"></i><b>5.4.3</b> Residual distributions and prediction intervals</a></li>
<li class="chapter" data-level="5.4.4" data-path="5-five.html"><a href="5-five.html#gps-accuracy"><i class="fa fa-check"></i><b>5.4.4</b> GPS accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-six.html"><a href="6-six.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-aa.html"><a href="A-aa.html"><i class="fa fa-check"></i><b>A</b> Code</a></li>
<li class="chapter" data-level="B" data-path="B-ab.html"><a href="B-ab.html"><i class="fa fa-check"></i><b>B</b> Models</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-ab.html"><a href="B-ab.html#bayview-model-point"><i class="fa fa-check"></i><b>B.1</b> Bayview model point</a></li>
<li class="chapter" data-level="B.2" data-path="B-ab.html"><a href="B-ab.html#downtown-model-point"><i class="fa fa-check"></i><b>B.2</b> Downtown model point</a></li>
<li class="chapter" data-level="B.3" data-path="B-ab.html"><a href="B-ab.html#residential-model-point"><i class="fa fa-check"></i><b>B.3</b> Residential model point</a></li>
<li class="chapter" data-level="B.4" data-path="B-ab.html"><a href="B-ab.html#presidio-model-point"><i class="fa fa-check"></i><b>B.4</b> Presidio model point</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="two" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Theoretical background</h1>
<p>This chapter presents a brief introduction to the theory of time series analysis. It is meant to serve as a theoretical foundation of the concepts used in this thesis. Therefore, the focus is clearly on those specific concepts, and the chapter should to no means be considered a thorough overview of the complete field of study.</p>
<p>The chapter is divided into five sections. In the first section, a formal definition of time series is given. The second section discusses the main characteristics of time series. Section three describes the different components of time series data and presents methods to split a time series into these components. The fourth section introduces statistical models to forecast time series, in particular the <em>autoregressive integrated moving average</em> (ARIMA) class of models. Finally, the fifth section focuses on a specific niche within time series analysis that is used in this thesis, namely time series clustering.</p>
<div id="time-series-definition" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Time series definition</h2>
<p>According to <span class="citation">(<strong>woodward2017?</strong>)</span>, a time series can be defined as follows:</p>
<p><strong>Definition 1</strong> A <em>time series</em> is a special type of a stochastic process. A stochastic process {<span class="math inline">\(Y(t); t \in T\)</span>} is a collection of random variables, where <span class="math inline">\(T\)</span> is an index set for which all of the random variables are defined on the same sample space. When <span class="math inline">\(T\)</span> represents time, the stochastic process is referred to as a time series. <span class="math inline">\(\blacksquare\)</span></p>
<p>Typically, observations are made at equally spaced time intervals, such as every hour, every day or every year. In such cases, <span class="math inline">\(T\)</span> takes on a discrete set of values, and are known as <em>discrete-time time series</em>. On the other hand, <em>continuous-time time series</em> arise when <span class="math inline">\(T\)</span> takes on a continuous range of values <span class="citation">(<strong>brockwell2002?</strong>)</span>. In this thesis, only discrete-time time series are analyzed.</p>
<p>An observed realization of the stochastic process described in Definition 1, is referred to by <span class="citation">(<strong>woodward2017?</strong>)</span> as a <em>realization of a time series</em>. Other works, such as <span class="citation">(<strong>brockwell2002?</strong>)</span>, <span class="citation">(<strong>shumway2011?</strong>)</span> and <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>, use the term <em>time series</em> for both the data and the stochastic process of which it is a realization. In this thesis, for the sake of simplicity, the latter approach is used, and no notational distinction is made.</p>
<p><strong>Definition 2</strong> A <em>time series</em> is a set of observed values {<span class="math inline">\(y_{t}\)</span>} of a stochastic process {<span class="math inline">\(Y(t); t \in T\)</span>}, where <span class="math inline">\(T\)</span> represents time. <span class="math inline">\(\blacksquare\)</span></p>
<p>From the context it will be clear whether the term time series refers to the process (Definition 1) or its realization (Definition 2). When clarification is needed, it is given locally.</p>
</div>
<div id="time-series-characteristics" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Time series characteristics</h2>
<div id="twotwoone" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Autocorrelation</h3>
<p>Analyzing time series raises unique problems in statistical modelling and inference. In conventional statistics, methods rely on the assumptions of independent and identically distributed random variables. However, in time series analysis, observations made at nearby moments in time are likely to be related. That is, it is likely that there exist internal relationships within a time series. If these relationships are linear, they are called autocorrelation, as defined in Definition 3 <span class="citation">(<strong>shumway2011?</strong>)</span>.</p>
<p><strong>Defintion 3</strong> <em>Autocorrelation</em> measures the linear correlation between two points on the same time series observed at different times. <span class="math inline">\(\blacksquare\)</span></p>
<p>Given a sample {<span class="math inline">\(y_{1}, y_{2}, ... , y_{n}\)</span>} of a time series, the degree of dependence in the data can be assessed by computing the <em>autocorrelation function</em> (ACF), for each lag <span class="math inline">\(h\)</span>, as defined in Equation <a href="2-two.html#eq:eq1">(2.1)</a> <span class="citation">(<strong>brockwell2002?</strong>)</span>.</p>
<p><span class="math display" id="eq:eq1">\[\begin{equation}
\hat\rho(h) = \frac{\hat\gamma(h)}{\hat\gamma(0)}, \quad -n &lt; h &lt; n
\tag{2.1}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\hat\gamma(h)\)</span> is the autocovariance function of the sample at lag <span class="math inline">\(h\)</span>, defined in Equation <a href="2-two.html#eq:eq2">(2.2)</a>.</p>
<p><span class="math display" id="eq:eq2">\[\begin{equation}
\hat\gamma(h) = n^{-1}\sum_{t=1}^{n-|h|}(y_{t+|h|} - \bar{y})(y_{t} - \bar{y}), \quad -n &lt; h &lt; n
\tag{2.2}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(n\)</span> is the length of the sample, <span class="math inline">\(y_{t}\)</span> is the data value at time <span class="math inline">\(t\)</span>, <span class="math inline">\(y_{t-|h|}\)</span> is the data value at time <span class="math inline">\(t\)</span> minus <span class="math inline">\(|h|\)</span> time periods, and <span class="math inline">\(\bar{y}\)</span> is the mean of the sample. Whenever a time series {<span class="math inline">\(Y_{t}\)</span>} is stationary, a concept that is introduced in the next section, the ACF of the sample {<span class="math inline">\(y_{t}\)</span>} can be used as an estimate for the ACF of {<span class="math inline">\(Y_{t}\)</span>}. A time series without autocorrelation, with zero mean and finite variance, is called <em>white noise</em>.</p>
<p>Conventional statistical methods can not be appropriately applied to data that exhibit autocorrelation, since the independency assumption is violated. The primary objective of time series analysis, therefore, is to develop mathematical models that are appropriate to use for data with a temporal autocorrelation structure <span class="citation">(<strong>shumway2011?</strong>)</span>. Furthermore, autocorrelation is in some cases an advantage, since an internal dependence structure implies that past observations can be used adequately to forecast future values.</p>
</div>
<div id="stationarity" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Stationarity</h3>
<p>In time series analysis, a key role is played by time series whose statistical properties do not vary with time <span class="citation">(<strong>brockwell2002?</strong>)</span>. Such time series are called <em>stationary time series</em>. The most restrictive form of stationarity is defined in Definition 4 <span class="citation">(<strong>woodward2017?</strong>)</span>.</p>
<p><strong>Definition 4</strong> A time series {<span class="math inline">\(Y(t); t \in T\)</span>} is <em>strictly stationary</em> if for any <span class="math inline">\(t_{1}, t_{2}, ..., t_{k} \in T\)</span> and any <span class="math inline">\(h \in T\)</span>, the joint distribution of {<span class="math inline">\(Y_{t_{1}}, Y_{t_{2}}, ..., Y_{t_{k}}\)</span>} is identical to that of {<span class="math inline">\(Y_{t_{1+h}}, Y_{t_{2+h}}, ..., Y_{t_{k+h}}\)</span>}. <span class="math inline">\(\blacksquare\)</span></p>
<p>However, it is often too hard to mathematically establish the requirement of strict stationarity, since the involved distributions are not known. For most applications, a milder form of stationarity, which only imposes conditions on the first and second moment of a time series, is sufficient. This is officially known as <em>weak stationarity</em>, but, in time series analysis, usually just called <em>stationarity</em>, for the sake of simplicity. It is mathematically defined as in Definition 5 <span class="citation">(<strong>woodward2017?</strong>)</span>.</p>
<p><strong>Definition 5</strong> A time series {<span class="math inline">\(Y(t); t \in T\)</span>} is <em>stationary</em> if</p>
<p><span class="math inline">\(\quad \text{1) } E[Y_{t}] = \mu \quad (\text{constant for all }t)\)</span>.</p>
<p><span class="math inline">\(\quad \text{2) } Var[Y_{t}] = \sigma^{2} \quad (\text{constant for all }t)\)</span>.</p>
<p><span class="math inline">\(\quad \text{3) } \gamma(t_{1}, t_{2}) \text{ depends only on } t_{2} - t_{1}\)</span>. <span class="math inline">\(\blacksquare\)</span></p>
<p>In words, this means that a time series is said to be stationary when the mean and variance are constant over time, and the autocovariance function only depends on the difference between two time points, and not on the time point itself. Note here that each time series that is strictly stationary (Definition 4), is, by definition, also stationary (Definition 5).</p>
<p>Stationarity is important, since in real-world problems, it is common that only one realization of a time series is available. That means that each random variable in the time series is represented by a single value. This makes it impossible to get an understanding of the underlying probability distributions of those random variables, unless it is assumed that their statistical properties are the same (i.e. the time series is stationary). In that case, the statistical properties of the whole sample can be used to estimate the statistical properties of each individual probability distribution. An understanding of the underlying probability distributions, in turn, is especially important when the goal is to forecast how the time series will behave at future time points. When the statistical properties of the time series have been constant over time in the past, one can simply predict that they will remain constant in the future.</p>
<p>In most practical applications, non-stationary time series are the rule rather than the exception. Luckily, by applying mathematical transformations, it often possible to render a non-stationary time series as, approximately, stationary. This process is referred to as <em>stationarizing</em> a time series <span class="citation">(<strong>nau2018?</strong>)</span>. Stationarizing is used often in statistical forecasting methods, which are discussed in Section <a href="2-two.html#twofour">2.4</a>.</p>
</div>
<div id="twotwothree" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Spectral entropy</h3>
<p>Often, two separate approaches to time series analysis are defined. The first, referred to as the <em>time domain</em>, deals primarily with the internal dependence structure in time series data, where current values can be explained in terms of a dependence on past values, as discussed in Section <a href="2-two.html#twotwoone">2.2.1</a>. The second, referred to as the <em>frequency domain</em>, works with a spectral representation of the time series, in which the original data is expressed as a weighted sum of sine and cosine waveforms, each with their own frequency. Named after the French mathematician Jean-Baptiste Joseph Fourier, such a representation is commonly known as a <em>Fourier representation</em> or <em>Fourier series</em>, and its corresponding sine and cosine terms as <em>Fourier terms</em>. Forecasting is inherently tied to the time domain of time series analysis, which will therefore be the focus of this thesis. However, there is no schism dividing the two approaches. That is, some frequency domain techniques can be useful even in the time domain, and vice versa <span class="citation">(<strong>shumway2011?</strong>)</span>.</p>
<p>One of such techniques is the calculation of the spectral entropy of a time series, which describes the order and regularity of a time series, based on its Fourier representation. The spectral entropy of a time series can be calculated with Equation <a href="2-two.html#eq:eq3">(2.3)</a>.</p>
<p><span class="math display" id="eq:eq3">\[\begin{equation}
H = \int_{-\pi}^{\pi} \hat{f}(\lambda)\log \hat{f}(\lambda)d\lambda
\tag{2.3}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\hat{f}_{y}(\lambda)\)</span> is the estimated spectral density function, which describes the importance of the different frequencies in the Fourier representation of the time series. Usually, <span class="math inline">\(H\)</span> is normalized to the range of values between 0 and 1. For a detailed description of the calculations, see <span class="citation">(<strong>goerg2013?</strong>)</span>.</p>
<p>Spectral entropy is useful in forecasting, since it can serve as a quantitative measure of the forecastability of a time series. Very smooth data, that are easy to forecast, will have a small value of <span class="math inline">\(H\)</span>, while very noisy data, that are hard to forecast, will have a large value of <span class="math inline">\(H\)</span> <span class="citation">(<strong>talagala2018?</strong>)</span>.</p>
</div>
</div>
<div id="time-series-components" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Time series components</h2>
<div id="definitions" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Definitions</h3>
<p>A time series can consist of various underlying patterns. Each of those patterns is considered a distinct component of the time series, with its own properties and behaviour. Splitting a time series into its components, is known as <em>time series decomposition</em>. It enables a separate analysis of all the components, which helps to better understand the dynamics of a time series, but can also be useful in forecasting, as will be discussed later in this chapter.</p>
<p><span class="citation">(<strong>hyndman2018fpp?</strong>)</span> define three main components of a time series: a trend-cycle component, a seasonal component, and a remainder component. For simplicity, the trend-cycle component is usually called just the trend component, which is done in this thesis as well.</p>
<p><strong>Definition 4</strong> The <em>trend component</em> is the combination of the trend and cyclical pattern of a time series. A trend exists when there is a long-term, not necessarily linear, increase or decrease in the data. A cyclical pattern occurs when the data exhibit rises and falls that are not of a fixed frequency. <span class="math inline">\(\blacksquare\)</span></p>
<p><strong>Definition 5</strong> The <em>seasonal component</em> contains the seasonal pattern of a time series. A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the the day of the week, and is always of a fixed and known frequency. <span class="math inline">\(\blacksquare\)</span></p>
<p><strong>Definition 6</strong> The <em>remainder component</em> is the remaining variation in a time series after the trend and seasonal components are removed. <span class="math inline">\(\blacksquare\)</span></p>
<p>There exist several methods for the decomposition of a time series. Most of them are based on the classical decomposition method, which is discussed in the next section. A more sophisticated approach is known as STL, and is covered in Section <a href="2-two.html#twothreethree">2.3.3</a>.</p>
</div>
<div id="classical-decomposition" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Classical decomposition</h3>
<p>The oldest and simplest method for the decomposition of a time series is referred to as classical decomposition by <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>. They present a stepwise approach for the use of the method, which is summarized in this section. Classical decomposition can be applied in two different forms. In the additive form, a time series is assumed to be the sum of its components, as shown in Equation <a href="2-two.html#eq:eq4">(2.4)</a>.</p>
<p><span class="math display" id="eq:eq4">\[\begin{equation}
y_{t} = T_{t} + S_{t} + R_{t}
\tag{2.4}
\end{equation}\]</span></p>
<p>In the multiplicative form, a time series is assumed to be the product of its components, as shown in Equation <a href="2-two.html#eq:eq5">(2.5)</a>.</p>
<p><span class="math display" id="eq:eq5">\[\begin{equation}
y_{t} = T_{t} \times S_{t} \times R_{t}
\tag{2.5}
\end{equation}\]</span></p>
<p>Where, for both Equation <a href="2-two.html#eq:eq4">(2.4)</a> and Equation <a href="2-two.html#eq:eq5">(2.5)</a>, <span class="math inline">\(y_{t}\)</span> is the data, <span class="math inline">\(T_{t}\)</span> is the trend component, <span class="math inline">\(S_{t}\)</span> is the seasonal component and <span class="math inline">\(R_{t}\)</span> is the remainder component.</p>
<p>Additive decomposition is used in cases when the amplitude of the variation around the trend is relatively constant. On the other hand, when the amplitude of the variation around the trend changes with the level of the trend, multiplicative decomposition should be used.</p>
<p>In both the additive and multiplicative form of classical decomposition, the first step is to estimate the trend component. This is done by smoothing the data with a symmetric moving average filter of order <span class="math inline">\(m\)</span>, where <span class="math inline">\(m\)</span> is a non-negative integer. That is, the estimate of the trend component at time <span class="math inline">\(t\)</span> is the average of all the data values within a window of <span class="math inline">\(m\)</span> time periods centered at <span class="math inline">\(t\)</span>, as shown in Equation <a href="2-two.html#eq:eq6">(2.6)</a>. Usually, <span class="math inline">\(m\)</span> is set to be equal to the seasonal period of the time series, which, in turn, is the number of observations per seasonal cycle. For example, when working with daily data that show a weekly seasonal pattern, the seasonal period is 7.</p>
<p><span class="math display" id="eq:eq6">\[\begin{equation}
\hat{T}_{t} = \frac{1}{m}\sum_{j=-k}^{k}y_{t+j}
\tag{2.6}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(k = (m-1)/2\)</span>.</p>
<p>The detrended time series data are then calculated by removing the estimated trend component from the original data. In the case of additive decomposition by subtraction, <span class="math inline">\(y_{t} - \hat{T}_{t}\)</span>, and in the case of multiplicative decomposition by division, <span class="math inline">\(y_{t}/\hat{T}_{t}\)</span>.</p>
<p>The seasonal component is estimated by averaging the detrended data values per season, as shown in Equation <a href="2-two.html#eq:eq7">(2.7)</a>. Using again the example of daily data with a weekly seasonal pattern, that would mean that the estimated seasonal component for a specific Monday is the average value of all Monday observations in the data set, the estimated seasonal component for a specific Tuesday is the average value of all Tuesday observations in the data set, and so on.</p>
<p><span class="math display" id="eq:eq7">\[\begin{equation}
\hat{S}_{t} = \frac{1}{n_{t}}\sum_{i=1}^{n_{t}}(\omega_{t})_{i}
\tag{2.7}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\omega_{t}\)</span> is a vector containing all the detrended values belonging to the same season as <span class="math inline">\(y_{t}\)</span>, and <span class="math inline">\(n_{t}\)</span> is the length of <span class="math inline">\(\omega_{t}\)</span>. Usually, the estimated seasonal component values for each season are adjusted such that they add up to 0 in the case of additive decomposition and 1 in the case of multiplicative decomposition.</p>
<p>Finally, the remainder component is estimated by removing both the estimated trend component and the estimated seasonal component from the original time series. For additive decomposition, this is done by applying Equation <a href="2-two.html#eq:eq8">(2.8)</a>.</p>
<p><span class="math display" id="eq:eq8">\[\begin{equation}
\hat{R}_{t} = y_{t} - \hat{T}_{t} - \hat{S}_{t}
\tag{2.8}
\end{equation}\]</span></p>
<p>For multiplicative decomposition, Equation <a href="2-two.html#eq:eq9">(2.9)</a> is used.</p>
<p><span class="math display" id="eq:eq9">\[\begin{equation}
\hat{R}_{t} = \frac{y_{t}}{\hat{T}_{t}\hat{S}_{t}}
\tag{2.9}
\end{equation}\]</span></p>
<p>Classical decomposition is generally praised for its simplicity, but has several disadvantages compared to some of the more modern decomposition methods <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>. As a consequence of the use of a symmetric moving average filter, there are no trend component estimates available for the first few and last few observations of the time series. Therefore, also the remainder component estimate lacks these values. An unknown current value of a time series, is mainly problematic when forecasting, as is shown in Section <a href="2-two.html#twofour">2.4</a>. Furthermore, the seasonal component stays constant over all the seasonal cycles, and cannot capture slight changes over time. Especially when working with longer time series, this may be an inappropriate representation of the truth. Finally, classical decomposition is not robust to extreme values in the data.</p>
</div>
<div id="twothreethree" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> STL decomposition</h3>
<p>A widely used method that is based on classical decomposition, but deals with many of the limitations mentioned above, is known as STL. It stands for <em>A Seasonal-Trend decomposition procedure based on Loess</em>, and was developed by <span class="citation">(<strong>cleveland1990?</strong>)</span>. In this section, their methodology is summarized. STL estimates all three components for every observation in a time series, and can also handle missing values in the data. Both the trend and seasonal component are robust and not distorted by extreme values. Furthermore, the seasonal component is not fixed, but can vary slightly over time.</p>
<p>As its name already implies, STL is based on loess, also known as locally-weighted regression. Loess was developed by <span class="citation">(<strong>cleveland1988?</strong>)</span>, and is a non-parametric regression technique, often used for smoothing, that fits weighted least squares regression curves to local subsets of a data set. Joining them together forms the loess regression curve <span class="math inline">\(\hat{g}(x)\)</span>. More specifically, for each value of <span class="math inline">\(x\)</span>, <span class="math inline">\(\hat{g}(x)\)</span> is computed in the following way. First, a positive integer <span class="math inline">\(q\)</span> is chosen, which defines the neighbourhood width. That is, the <span class="math inline">\(q\)</span> observations that are closest to <span class="math inline">\(x\)</span> are selected as neighbours of <span class="math inline">\(x\)</span>. Each of these observations is given a weight based on its distance to x, in a way that the closest observations get the highest weight. Let <span class="math inline">\(W\)</span> be the tricube weight function as defined in Equation <a href="2-two.html#eq:eq10">(2.10)</a>.</p>
<p><span class="math display" id="eq:eq10">\[\begin{equation}
W(u) =
    \begin{cases}
      (1 - u^{3})^{3} &amp;\quad 0 \leq u &lt; 1\\
      0 &amp;\quad u \geq 1\\
    \end{cases}
\tag{2.10}
\end{equation}\]</span></p>
<p>Subsequently, the neighbourhood weight for each observation <span class="math inline">\(x_{i}\)</span> is calculated with Equation <a href="2-two.html#eq:eq11">(2.11)</a>.</p>
<p><span class="math display" id="eq:eq11">\[\begin{equation}
\upsilon_{i} = W\Bigg(\frac{|x_{i} - x|}{\lambda_{q}(x)}\Bigg)
\tag{2.11}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\lambda_{q}(x)\)</span> is the distance of the <span class="math inline">\(q\)</span>th farthest observation from <span class="math inline">\(x\)</span>. Then, <span class="math inline">\(\hat{g}(x)\)</span> is calculated by fitting a polynomial regression of degree <span class="math inline">\(d\)</span> to x, using weighted least squares with the neighbourhood weights <span class="math inline">\(\upsilon_{i}\)</span>. Usually, <span class="math inline">\(d\)</span> is either <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span>, corresponding respectively to a locally-linear regression and a locally-quadratic regression. Since the loess regression curve is smooth, there is no need to compute <span class="math inline">\(\hat{g}(x)\)</span> at all possible values of <span class="math inline">\(x\)</span>. In general, the computation of <span class="math inline">\(\hat{g}(x)\)</span> as described above is only performed at a finite set of locations, and interpolated elsewhere.</p>
<p>STL uses loess for several smoothing operations, that, when performed on a time series, lead to estimations of the trend, seasonal and remainder components of the data. The method is build up of two loops: an inner loop nested inside an outer loop. In the inner loop, the estimates of the seasonal and trend component are updated once, in a stepwise manner, which is described below.</p>
<p><strong>Step 1.</strong> The inner loop starts with computing the detrended time series data <span class="math inline">\(y_{t} - \hat{T}_{t}\)</span> from the original time series data <span class="math inline">\(y_{t}\)</span>. In the initial pass through the inner loop, there is no estimation of <span class="math inline">\(T_{t}\)</span> yet, and <span class="math inline">\(\hat{T}_{t}\)</span> is set equivalent to <span class="math inline">\(0\)</span>. That is, it is assumed there is no trend at all. This may be a rather poor estimate, but inside the loop, it will soon be updated to something more reasonable. In all successive passes through the loop, the estimated trend component that resulted from the previous loop is used.</p>
<p><strong>Step 2.</strong> In the second step, the detrended time series is split up into subsets, with each subset containing all the data belonging to one specific season. That is, there will be <span class="math inline">\(n_{p}\)</span> different subsets, where <span class="math inline">\(n_{p}\)</span> is the number of observations per seasonal cycle. Each of those subsets is smoothed by loess, with <span class="math inline">\(q = n_{s}\)</span> and <span class="math inline">\(d = 1\)</span>. <span class="math inline">\(n_{s}\)</span> is referred to as the seasonal smoothing parameter and its value must be chosen by the analyst. It basically determines how much the seasonal component is allowed to change over time. High values of <span class="math inline">\(n_{s}\)</span> allow little variation, while low values can lead to overfitting. The smoothed values of all the subsets are then binded back together into a temporary seasonal component <span class="math inline">\(C_{t}\)</span>. Each end of <span class="math inline">\(C_{t}\)</span> is extended <span class="math inline">\(n_{p}\)</span> positions, such that <span class="math inline">\(C_{t}\)</span> has <span class="math inline">\(2 \times n_{p}\)</span> observations more than the original time series.</p>
<p><strong>Step 3.</strong> In the third step, any trend that may have contaminated <span class="math inline">\(C_{t}\)</span> is identified. This is done by applying a sequence of smoothers, called a low-pass filter, to <span class="math inline">\(C_{t}\)</span>. It starts with a moving average of length <span class="math inline">\(n_{p}\)</span>, followed by another moving average of length <span class="math inline">\(n_{p}\)</span>, followed by a moving average of length 3, followed by a loess smoothing with <span class="math inline">\(q = n_{l}\)</span> and <span class="math inline">\(d = 1\)</span>. Just as earlier with <span class="math inline">\(n_{s}\)</span>, the low-pass filter smoothing parameter <span class="math inline">\(n_{l}\)</span> should be chosen by the analyst. The output of the third step is called <span class="math inline">\(L_{t}\)</span>. Since moving averages are used, the first <span class="math inline">\(n_{p}\)</span> observations and the last <span class="math inline">\(n_{p}\)</span> observations of <span class="math inline">\(C_{t}\)</span> will not have a smoothed value in <span class="math inline">\(L_{t}\)</span>. However, this was already accounted for by extending <span class="math inline">\(C_{t}\)</span> in step 2. That is, <span class="math inline">\(L_{t}\)</span> is again of the same length as the original time series.</p>
<p><strong>Step 4.</strong> In the fourth step, the seasonal component is estimated by detrending the temporary seasonal component. That is, <span class="math inline">\(\hat{S}_{t} = C_{t} - L_{t}\)</span>.</p>
<p><strong>Step 5.</strong> In the fifth step, the deseasonalized time series data <span class="math inline">\(y_{t} - \hat{S}_{t}\)</span> are computed from the original time series data <span class="math inline">\(y_{t}\)</span>.</p>
<p><strong>Step 6.</strong> In the sixth and last step of the inner loop, the estimation of the trend component, <span class="math inline">\(\hat{T}_{t}\)</span>, is calculated by loess smoothing the deseasonalized time series with <span class="math inline">\(q = n_{t}\)</span> and <span class="math inline">\(d = 1\)</span>. The trend smoothing parameter <span class="math inline">\(n_{t}\)</span> should be chosen by the analyst.</p>
<p>The outer loop of STL starts with <span class="math inline">\(n_{i}\)</span> iterations of the inner loop. The estimations of the trend and seasonal components that follow from the passes through the inner loop, are used to estimate the remainder component with Equation <a href="2-two.html#eq:eq12">(2.12)</a>.</p>
<p><span class="math display" id="eq:eq12">\[\begin{equation}
\hat{R}_{t} = y_{t} - \hat{T}_{t} - \hat{S}_{t}
\tag{2.12}
\end{equation}\]</span></p>
<p>For each observation in the time series, a robustness weight is calculated. This weight reflects how extreme the value of the remainder component of that observation is, in a way that an extreme value is given a very low, or even zero, weight. Let <span class="math inline">\(B\)</span> be the bisquare weight function as defined in Equation <a href="2-two.html#eq:eq13">(2.13)</a>.</p>
<p><span class="math display" id="eq:eq13">\[\begin{equation}
B(u) =
    \begin{cases}
      (1 - u^{2})^{2} &amp;\quad 0 \leq u &lt; 1\\
      0 &amp;\quad u \geq 1\\
    \end{cases}
\tag{2.13}
\end{equation}\]</span></p>
<p>Then, the robustness weight at time point <span class="math inline">\(t\)</span> is calculated with Equation <a href="2-two.html#eq:eq14">(2.14)</a>.</p>
<p><span class="math display" id="eq:eq14">\[\begin{equation}
\rho_{t} = B\Bigg(\frac{|R_{t}|}{6median(|R_{t}|)}\Bigg)
\tag{2.14}
\end{equation}\]</span></p>
<p>After the first pass through the outer loop, the next iteration starts again with <span class="math inline">\(n_{i}\)</span> passes through the inner loop. However, in the loess smoothing in step 2 and step 6, each neighbourhood weight <span class="math inline">\(\upsilon_{t}\)</span> is now multiplied by its corresponding robustness weight <span class="math inline">\(\rho_{t}\)</span>, such that extreme values have less influence on the estimates of the trend and seasonal components. Also, the estimated trend component that resulted from the last inner loop in the previous outer loop, is now used as first value of <span class="math inline">\(\hat{T}_{t}\)</span>, rather than <span class="math inline">\(0\)</span>. In total, the outer loop is carried out <span class="math inline">\(n_{o}\)</span> times.</p>
<p>STL is designed for additive decomposition. However, a multiplicative version can be obtained by first log transforming the data, and finally back-transforming the components. This is based on the logarithm product rule, which states that <span class="math inline">\(log(a) + log(b)\)</span> is equivalent to <span class="math inline">\(log(a \times b)\)</span> <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>.</p>
<p>The complete methodology of STL as described above is summarized in Figure <a href="2-two.html#fig:stl">2.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:stl"></span>
<img src="figures/STL.png" alt="Summary of the STL methodology" width="\textwidth" />
<p class="caption">
Figure 2.1: Summary of the STL methodology
</p>
</div>
</div>
</div>
<div id="twofour" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Time series forecasting</h2>
<div id="twofourone" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Forecasting models</h3>
<p>Often, the main aim of time series analysis is to forecast future values of a time series. In some cases, this can be done by using external exploratory variables. One could for example try to forecast the profit of ice cream sales by using air temperature as an exploratory variable in a linear regression model. However, there are several reasons not to forecast time series in this way, as summed up by <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>. Firstly, the underlying system of the forecasted time series may not be sufficiently understood, and even if it is, the relations with exploratory variables may be too complex. Secondly, when forecasting future values of a time series, also the future values of the exploratory variables should be known, which means that each exploratory variable should be forecasted separately before the response variable can be forecasted. This may be too difficult to do accurately, and even when it is possible, it remains a very time consuming task. Especially when the only aim is to know what will happen, and not why it will happen, it is not worth the effort. Finally, modelling a time series with conventional statistical method like linear regression will likely result in model errors that exhibit autocorrelation, which implies that such models are not able to capture all the dynamics of the data. Thus, produced forecast are not as efficient, and, probably, not as accurate as can be.</p>
<p>Instead, in time series analysis, the internal dependence structure of a time series is used to forecast future values as a function of the current and past values <span class="citation">(<strong>shumway2011?</strong>)</span>. Obviously, this primarily requires a good understanding of that structure, which is obtained by describing the process that generated the data with a time series model, as defined in Definition 7, adapted from <span class="citation">(<strong>brockwell2002?</strong>)</span>.</p>
<p><strong>Definition 7</strong> A <em>time series model</em> for an observed realization {<span class="math inline">\(y_{t}\)</span>} of a time series {<span class="math inline">\(Y_{t}\)</span>} is a specification of the joint distributions, or possibly only the means, variances and covariances, of the random variables that {<span class="math inline">\(Y_{t}\)</span>} comprises. <span class="math inline">\(\blacksquare\)</span></p>
<p>One of the most famous and widely used groups of time series models is known as the <em>autoregressive integrated moving average</em> (ARIMA) class of models, developed by <span class="citation">(<strong>box1970?</strong>)</span>. In this thesis, ARIMA is used as well. The next section gives a summary of its theory, based on <span class="citation">(<strong>brockwell2002?</strong>)</span>, Chapter 5, <span class="citation">(<strong>shumway2011?</strong>)</span>, Chapter 3, and <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>, Chapter 3 and 8.</p>
</div>
<div id="arima" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> ARIMA</h3>
<div id="structure" class="section level4" number="2.4.2.1">
<h4><span class="header-section-number">2.4.2.1</span> 2.4.2.1 Structure</h4>
<p>An ARIMA model is a combination of an <em>autoregressive</em> (AR) and <em>moving average</em> (MA) model, preceded by a differencing operation on the original data. An autoregressive model of order <span class="math inline">\(p\)</span>, commonly referred to as an AR(<span class="math inline">\(p\)</span>) model, is based on the assumption that the current value of a time series is a linear combination of <span class="math inline">\(p\)</span> previous values, as showed in Equation <a href="2-two.html#eq:eq15">(2.15)</a>.</p>
<p><span class="math display" id="eq:eq15">\[\begin{equation}
y_{t} = \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + ... + \phi_{p}y_{t-p} + \epsilon_{t}
\tag{2.15}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(y_{t}\)</span> is the current value of the time series at time period <span class="math inline">\(t\)</span>, <span class="math inline">\(\epsilon_{t}\)</span> is the random error (i.e. white noise) at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\phi_{1},...,\phi_{p}\)</span> are model parameters.</p>
<p>A moving average model of order <span class="math inline">\(q\)</span>, commonly referred to as an MA(<span class="math inline">\(q\)</span>) model, is based on the assumption that the current value of a time series is a linear combination of <span class="math inline">\(q\)</span> previous errors, as showed in Equation <a href="2-two.html#eq:eq16">(2.16)</a>.</p>
<p><span class="math display" id="eq:eq16">\[\begin{equation}
y_{t} = \epsilon_{t} + \theta_{1}\epsilon_{t-1} + \theta_{2}\epsilon_{t-2} + ... + \theta_{q}\epsilon_{t-q}
\tag{2.16}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(y_{t}\)</span> is the current value of the time series at time period <span class="math inline">\(t\)</span>, <span class="math inline">\(\epsilon_{t}\)</span> is the error at time period <span class="math inline">\(t\)</span>, which is assumed to be white noise, and <span class="math inline">\(\theta_{1},...,\theta_{q}\)</span> are model parameters.</p>
<p>AR(<span class="math inline">\(p\)</span>) and MA(<span class="math inline">\(q\)</span>) models can be combined into an autoregressive moving average model of order (<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>), commonly referred to as ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>). That is, in such a model, the current value of a time series is a linear combination of both <span class="math inline">\(p\)</span> previous values and <span class="math inline">\(q\)</span> previous errors, as showed in Equation <a href="2-two.html#eq:eq17">(2.17)</a>.</p>
<p><span class="math display" id="eq:eq17">\[\begin{equation}
y_{t} = \phi_{1}y_{t-1} + ... + \phi_{p}y_{t-p} + \theta_{1}\epsilon_{t-1} + ... + \theta_{q}\epsilon_{t-q} + \epsilon_{t}
\tag{2.17}
\end{equation}\]</span></p>
<p>ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>) models require the forecasted time series to be stationary. When working with non-stationary time series, it is often possible to stationarize the series by differencing it one or more times. The first order difference of a time series is the series of changes from one time period to the next, as shown in Equation <a href="2-two.html#eq:eq18">(2.18)</a>.</p>
<p><span class="math display" id="eq:eq18">\[\begin{equation}
\nabla y_{t} = y_{t} - y_{t-1}
\tag{2.18}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\nabla y_{t}\)</span> is the first order difference of <span class="math inline">\(y_{t}\)</span>. When the first order difference is still non-stationary, the second order difference <span class="math inline">\(\nabla^{2}y_{t}\)</span> can be computed by taking again the first order difference of <span class="math inline">\(\nabla y_{t}\)</span>, and so on.</p>
<p>The original non-stationary time series that needed to be differenced in order to get stationary, is called an <em>integrated</em> version of the stationary series. That is why a model that first stationarizes the data by applying a <span class="math inline">\(d\)</span>-th order difference, before fitting an ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>) model, is called an autoregressive integrated moving average model of order (<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>), commonly referred to as ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>). That is, in such a model, the current value of the <span class="math inline">\(d-\)</span>th order difference of a time series is a linear combination of both <span class="math inline">\(p\)</span> previous values and <span class="math inline">\(q\)</span> previous errors, as showed in Equation <a href="2-two.html#eq:eq19">(2.19)</a>.</p>
<p><span class="math display" id="eq:eq19">\[\begin{equation}
\nabla^{d}y_{t} = \phi_{1}\nabla^{d}y_{t-1} + ... + \phi_{p}\nabla^{d}y_{t-p} + \theta_{1}\epsilon_{t-1} + ... + \theta_{q}\epsilon_{t-q} + \epsilon_{t}
\tag{2.19}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\nabla^{d}y_{t}\)</span> is the <span class="math inline">\(d\)</span>-th order difference of <span class="math inline">\(y_{t}\)</span>. Note here that ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) is a general form of all the other models discussed earlier in this section. For example, an AR(1) model can also be written as ARIMA(1,0,0), an ARMA(2,1) as ARIMA(1,0,2), and so on.</p>
<p>The process of finding an appropriate ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model that represents a time series is known as the Box-Jenkins modelling procedure and consists of three stages, named model selection, parameter estimation and model checking. All these stages are described separately in the next three subsections.</p>
</div>
<div id="twofourtwotwo" class="section level4" number="2.4.2.2">
<h4><span class="header-section-number">2.4.2.2</span> 2.4.2.2 Model selection</h4>
<p>In the model selection stage, <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are chosen. In this process, <span class="math inline">\(d\)</span> is selected first, such that the choice of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> will be based on a stationary time series. An appropriate value for <span class="math inline">\(d\)</span> can be found by inspecting the plotted data <span class="math inline">\(y_{t}\)</span>, with time on the x-axis, and define visually if the data are stationary. If not, then difference the data once, and inspect the plot of <span class="math inline">\(\nabla y_{t}\)</span>. If <span class="math inline">\(\nabla y_{t}\)</span> does not seem stationary either, take the second-order difference <span class="math inline">\(\nabla^{2} y_{t}\)</span>, and so on. In general, however, it is not recommended to difference more than two times.</p>
<p>As an addition to the time plots, plotting the sample autocorrelation function of the data can help to identify stationarity. Non-stationary data show a slow decay in autocorrelation as the time lag increases, while for stationary data, the autocorrelation will drop to zero relatively fast.</p>
<p>Once <span class="math inline">\(d\)</span> has been set, either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> can be selected by inspecting the autocorrelation function plot and the partial autocorrelation function plot of the differenced data, which respectively plot the sample autocorrelation function (ACF), defined in Equation <a href="2-two.html#eq:eq1">(2.1)</a>, and the sample partial autocorrelation function (PACF), for several lags <span class="math inline">\(h\)</span>. The PACF is the relationship between an observation at time <span class="math inline">\(t\)</span> and and observation at time <span class="math inline">\(t-k\)</span>, removing the effects of all time lags in between, i.e. <span class="math inline">\(1, 2, ..., k-1\)</span>. Then, appropriate values for either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> are found with the following rules of thumb:</p>
<ul>
<li>The PACF plot of an ARIMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(d\)</span>,<span class="math inline">\(0\)</span>) process cuts of after lag <span class="math inline">\(p\)</span>; the ACF plot tails off.</li>
<li>The ACF plot of an ARIMA(<span class="math inline">\(0\)</span>,<span class="math inline">\(d\)</span>,<span class="math inline">\(q\)</span>) process cuts of after lag <span class="math inline">\(q\)</span>; the PACF plot tails off.</li>
</ul>
<p>When dealing with ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) processes where both <span class="math inline">\(p &gt; 0\)</span> and <span class="math inline">\(q &gt; 0\)</span>, the ACF plot and PACF plot will both tail off, and finding appropriate values for <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> turns into a trial-and-error approach, where models with different combinations of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are compared.</p>
<p>The methodology as described above is used often, but involves a lot of manual interventions. This makes it a rather subjective way of working, that is labour intensive, especially when a large number of time series needs to be modelled, and requires expert knowledge. Therefore, several automated approaches to select <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> have been proposed. One of them is the Hyndman-Khandakar algorithm, which methodology is summarized below, in a simplified way. For the full details, see <span class="citation">(<strong>forecast?</strong>)</span>.</p>
<p><strong>Step 1.</strong> To define <span class="math inline">\(d\)</span>, the Hyndman-Khandakar algorithm uses the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which is a statistical test used to determine stationarity of a time series. Only if there is enough statistical evidence, the null hypothesis that the time series is stationary will be rejected, and the time series is instead considered to be non-stationary. The detailed mathematics underlying the test can be found in <span class="citation">(<strong>kwiat1992?</strong>)</span>.</p>
<p>Using the KPSS test, first, the original data <span class="math inline">\(y_{t}\)</span> are tested for stationarity. When <span class="math inline">\(y_{t}\)</span> are considered stationary, <span class="math inline">\(d = 0\)</span>, and when considered non-stationary, the first-order differenced data <span class="math inline">\(\nabla y_{t}\)</span> are tested for stationarity. Again, when <span class="math inline">\(\nabla y_{t}\)</span> are considered stationary, <span class="math inline">\(d = 1\)</span>, and when considered non-stationary, the second-order differenced data <span class="math inline">\(\nabla^{2} y_{t}\)</span> are tested for stationarity. This process is repeated until a stationary series is obtained.</p>
<p><strong>Step 2.</strong> In the second step, four models are fitted to the <span class="math inline">\(d\)</span>-times differenced data.</p>
<ul>
<li>An ARIMA(<span class="math inline">\(0\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(0\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(1\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(0\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(0\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(1\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(2\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(2\)</span>) model.</li>
</ul>
<p>The model with the lowest AIC is selected. AIC, which stands for Aikake’s Information Criterion, is a measure for the goodness-of-fit of a model, and can be calculated with Equation <a href="2-two.html#eq:eq20">(2.20)</a>.</p>
<p><span class="math display" id="eq:eq20">\[\begin{equation}
AIC = -2 \log(L) + 2k
\tag{2.20}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(L\)</span> is the Gaussian likelihood of the data, and <span class="math inline">\(k\)</span> is the number of free parameters in the model. In this case, <span class="math inline">\(k = p + q + l + 1\)</span>, where <span class="math inline">\(l = 1\)</span> when a non-zero constant is included, and <span class="math inline">\(l = 0\)</span> otherwise. The ‘<span class="math inline">\(+1\)</span>’ term is included, since the variance of the residuals is also a parameter. To find the best fitting model, AIC should be minimized. The idea behind AIC is the following. The likelihood monotonically increases when more parameters are added to the model, and therefore, only maximizing the likelihood would favor a model that overfits the data. AIC prevents such overfitting, by penalizing the likelihood with a term that is proportional to the number of parameters used in the model.</p>
<p><strong>Step 3.</strong> In the third step, several variations of the model that was selected in step 2, are fitted to the <span class="math inline">\(d\)</span>-times differenced data. These variations include:</p>
<ul>
<li>Models where either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> vary <span class="math inline">\(\pm 1\)</span> from the selected model, given that <span class="math inline">\(p, q \ngtr 5\)</span>.</li>
<li>Models where both <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> vary <span class="math inline">\(\pm 1\)</span> from the selected model, given that <span class="math inline">\(p, q \ngtr 5\)</span>.</li>
</ul>
<p>From the selected model and all its variations, the model with the lowest AIC is chosen to be the new selected model, and step 3 is repeated. The algorithm stops when there are no variations of the selected model that have a lower AIC. In that case, the selected model is the optimal model, and forms the output of the Hyndman-Khandakar algorithm. The complete methodology of the algorithm as described above is summarized in Figure <a href="2-two.html#fig:hyndman">2.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:hyndman"></span>
<img src="figures/hyndman.png" alt="Summary of the Hyndman-Khandakar algorithm" width="\textwidth" />
<p class="caption">
Figure 2.2: Summary of the Hyndman-Khandakar algorithm
</p>
</div>
</div>
<div id="parameter-estimation" class="section level4" number="2.4.2.3">
<h4><span class="header-section-number">2.4.2.3</span> 2.4.2.3 Parameter estimation</h4>
<p>When <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are defined, the model parameters <span class="math inline">\(\phi_{1},...,\phi_{p}\)</span> and <span class="math inline">\(\theta_{1},...,\theta_{q}\)</span> need to be estimated. Usually, this is done with <em>maximum likelihood estimation</em> (MLE). The likelihood is the probability of obtaining the observed data, given the model and specific parameter values. The parameter values that maximize the likelihood are called the maximum likelihood estimators of the true parameters, and will be used as the parameter estimates in the fitted ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model, which then is referred to as the maximum likelihood ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model. The detailed mathematical description of MLE for ARIMA models can be found in <span class="citation">(<strong>brockwell2002?</strong>)</span>, section 5.2.</p>
<p>Note here that the Hyndman-Khandakar algorithm already produces a fitted model as output, and the parameter estimation as described in this section is done inside the algorithm, each time a model is fitted to the <span class="math inline">\(d\)</span>-times differenced data.</p>
</div>
<div id="twofourtwofour" class="section level4" number="2.4.2.4">
<h4><span class="header-section-number">2.4.2.4</span> 2.4.2.4 Model checking</h4>
<p>Model checking involves identifying if the fitted model is adequate. This is done by inspecting its residuals, which are defined as the difference between the actual observations and the corresponding fitted values, as shown in Equation <a href="2-two.html#eq:eq21">(2.21)</a>.</p>
<p><span class="math display" id="eq:eq21">\[\begin{equation}
\epsilon_{t} = y_{t} - \hat{y}_{t}
\tag{2.21}
\end{equation}\]</span></p>
<p>If the maximum likelihood ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model is the true process that generated the data, the residuals should be completely white noise. Recall, however, that the model is an estimation of the true process. Therefore, a good model that fits the data well, should have residuals with properties that <em>approximately</em> reflect those of white noise, i.e. a zero mean and no autocorrelation. If autocorrelation is present in the residuals, this means that there is still information left in the data, which could be used to create more accurate forecasts. A non-zero mean will lead to biased forecasts.</p>
<p>Autocorrelation in the residuals can be detected by a visual interpretation of the residual ACF plot, which will always show some autocorrelation, due to random variation. Therefore, given that <span class="math inline">\(n\)</span> is the length of the modelled time series, and assuming a normal distribution, the residuals are considered to be uncorrelated when for at least 95% of the time lags, the residual ACF lies within the interval <span class="math inline">\([-1.96/\sqrt{n}, 1.96/\sqrt{n}]\)</span>.</p>
<p>Usually, several computations within the model fitting and forecasting process build on the assumption that the data come from a normally distributed population. For example, in MLE and the calculation of AIC, Gaussian likelihood is commonly used. Furthermore, prediction intervals of forecasts are in general derived by assuming a normal distribution. Normally distributed residuals indicate that these assumptions were valid, and are therefore a valuable property of a model. However, as stated by <span class="citation">(<strong>brockwell2002?</strong>)</span>, using Gaussian likelihood is sensible even when the data are not normally distributed.</p>
</div>
<div id="forecasting" class="section level4" number="2.4.2.5">
<h4><span class="header-section-number">2.4.2.5</span> 2.4.2.5 Forecasting</h4>
<p>A fitted ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model can then be used to forecast the future values of a time series. To do so, Equation <a href="2-two.html#eq:eq19">(2.19)</a> is rewritten, such that the current value of the time series, <span class="math inline">\(y_{t}\)</span>, is replaced by a future value of the time series, <span class="math inline">\(y_{t+h}\)</span>, as showed in Equation <a href="2-two.html#eq:eq22">(2.22)</a>.</p>
<p><span class="math display" id="eq:eq22">\[\begin{equation}
\nabla^{d}y_{t+h} = \hat\phi_{1}\nabla^{d}y_{t+h-1} + ... + \hat\phi_{p}\nabla^{d}y_{t+h-p} +\hat\theta_{1}\epsilon_{t+h-1} + ... + \hat\theta_{q}\epsilon_{t+h-q} + \epsilon_{t+1}
\tag{2.22}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(h\)</span> is the forecast horizon, i.e. the number of time lags ahead at which the forecast is made, <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are known and constant, and <span class="math inline">\(\hat\phi_{1},...,\hat\phi_{p}\)</span> and <span class="math inline">\(\hat\theta_{1},...,\hat\theta_{q}\)</span> are the estimated parameter values, which are also constant.</p>
<p>When <span class="math inline">\(h &gt; 1\)</span>, more than one forecast has to be made. For example, the forecast of <span class="math inline">\(\nabla^{d}y_{t+2}\)</span>, the value of the time series two time lags ahead, is based on <span class="math inline">\(\nabla^{d}y_{t+2-1}\)</span>, the value of the time series one time lag ahead. Therefore, <span class="math inline">\(\nabla^{d}y_{t+2-1}\)</span> needs to be forecasted first, before <span class="math inline">\(\nabla^{d}y_{t+2}\)</span> can be forecasted. In general, this means that the uncertainty of the forecasts increases as <span class="math inline">\(h\)</span> increases. This uncertainty is expressed by means of a prediction interval. Most often, the 95% prediction interval is used. Assuming normally distributed residuals, the lower and upper bound of the 95% prediction interval for the <span class="math inline">\(h\)</span>-step forecast can be calculated with Equation <a href="2-two.html#eq:eq23">(2.23)</a> and Equation <a href="2-two.html#eq:eq24">(2.24)</a>, respectively.</p>
<p><span class="math display" id="eq:eq23">\[\begin{equation}
\ell = \hat{y}_{t+h} - 1.96\hat\sigma_{h}
\tag{2.23}
\end{equation}\]</span>
<span class="math display" id="eq:eq24">\[\begin{equation}
\upsilon = \hat{y}_{t+h} + 1.96\hat\sigma_{h}
\tag{2.24}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\ell\)</span> is the lower bound of the 95% prediction interval, <span class="math inline">\(\upsilon\)</span> is the upper bound of the 95% prediction interval, <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted value <span class="math inline">\(h\)</span> time lags ahead. and <span class="math inline">\(\hat\sigma_{h}\)</span> is the estimated standard deviation of the forecast distribution <span class="math inline">\(h\)</span> time lags ahead, which is explained below. The 95% prediction interval can be interpreted as follows: there is a 95% probability that <span class="math inline">\(\ell \leq {y}_{t+h} \leq \upsilon\)</span>.</p>
<p>Recall that in Definition 1, a time series was defined as a collection of random variables. In fact, to state it statistically correct, it is the distribution of the random variable <span class="math inline">\(h\)</span> time lags ahead that is forecasted, rather than an individual value. This distribution is referred to as the forecast distribution, and the single forecasted value, also known as the <em>point forecast</em>, is then taken to be the mean of the forecast distribution. In Equation <a href="2-two.html#eq:eq23">(2.23)</a> and Equation <a href="2-two.html#eq:eq24">(2.24)</a>, <span class="math inline">\(\hat\sigma_{h}\)</span> is the estimated standard deviation of the forecasted distribution, assuming it is a normal distribution with mean <span class="math inline">\({y}_{t+h}\)</span> and variance <span class="math inline">\(\sigma_{h}^{2}\)</span>. When <span class="math inline">\(h = 1\)</span>, the residual standard deviation <span class="math inline">\(\sigma_{\epsilon}\)</span> is a good estimate for <span class="math inline">\(\sigma_{h}\)</span>. However, for <span class="math inline">\(h &gt; 1\)</span>, computations get more complex. For a detailed description, see <span class="citation">(<strong>shumway2011?</strong>)</span>, Section 3.5.</p>
</div>
<div id="twofourtwosix" class="section level4" number="2.4.2.6">
<h4><span class="header-section-number">2.4.2.6</span> 2.4.2.6 Accuracy evaluation</h4>
<p>A good model fit, does not necessarily lead to accurate forecasts. Therefore, when evaluating its performance, the forecasting model should be used to forecast multiple values of new data that were not included in the model building process. The error of each individual forecast can be calculated with Equation <a href="2-two.html#eq:eq25">(2.25)</a>.</p>
<p><span class="math display" id="eq:eq25">\[\begin{equation}
e_{t+h} = y_{t+h} - \hat{y}_{t+h}
\tag{2.25}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(y_{t+h}\)</span> is the observed data value <span class="math inline">\(h\)</span> time lags into the future, and <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted data value <span class="math inline">\(h\)</span> time lags into the future. Obviously, future in this sense is relative to the model building period.</p>
<p>When making <span class="math inline">\(k\)</span> different forecasts, the corresponding forecast errors <span class="math inline">\(e_{1}, e_{2}, ..., e_{k}\)</span>, can be summarized with an error metric. Several of those metrics exist. Some of them are only applicable to errors that all have the same units, while others may also be used when errors with different units are compared. Since all forecasts in this thesis are distances, the unit-dependent errors are adequate. Most commonly used are the Mean Absolute Error (MAE), which can be calculated with Equation <a href="2-two.html#eq:eq26">(2.26)</a>, and the Root Mean Squared Error (RMSE), which can be calculated with Equation <a href="2-two.html#eq:eq27">(2.27)</a>.</p>
<p><span class="math display" id="eq:eq26">\[\begin{equation}
MAE = \frac{\sum_{i = 1}^k |e_{i}|}{k}
\tag{2.26}
\end{equation}\]</span>
<span class="math display" id="eq:eq27">\[\begin{equation}
RMSE = \sqrt{\frac{\sum_{i = 1}^k e_{i}^{2}}{k}}
\tag{2.27}
\end{equation}\]</span></p>
<p>Both the MAE and RMSE return values that are in the same scale as the original data. The MAE gives the same weight to all errors. The RMSE, however, gives large errors more weight than small errors, and therefore penalizes a large error variance. According to <span class="citation">(<strong>chai2014?</strong>)</span>, the RMSE usually is better at revealing differences in model performance.</p>
<p>In some of the works discussed in Section <a href="1-introduction.html#onethree">1.3</a>, such as <span class="citation">(<strong>li2015?</strong>)</span>, <span class="citation">(<strong>yang2016?</strong>)</span> and <span class="citation">(<strong>lozano2018?</strong>)</span>, the Root Mean Squared Logarithmic Error (RMSLE) is reported instead of the RMSE. Here, the natural logarithms of the observed and forecasted data values are used in the forecast error computation. The main reason for doing so, is that larger errors, usually occurring during peak hours or in areas/stations with a high usage intensity, do not dominate smaller errors.</p>
</div>
<div id="twofourtwoseven" class="section level4" number="2.4.2.7">
<h4><span class="header-section-number">2.4.2.7</span> 2.4.2.7 Transformations</h4>
<p>Often, forecasts can be improved by using mathematical transformations, such that the original data are adjusted for some known patterns causing non-stationary and/or non-linear behaviour. That is, the data are transformed in advance, and the modelling and forecasting procedures are applied to the transformed data. After forecasting the transformed data, forecasted values on the original scale are obtained based upon the inverse transformation, a process that is commonly called <em>back transforming</em>. A particularly useful transformation is the <em>log transformation</em>, which suppresses larger fluctuations that occur when the level of the time series increases. Furthermore, they guarantee strictly positive forecasted values. The log transformed data <span class="math inline">\(\omega_{t}\)</span>, is derived by taking the natural logarithm of the original data <span class="math inline">\(y_{t}\)</span>, as showed in Equation <a href="2-two.html#eq:eq28">(2.28)</a>.</p>
<p><span class="math display" id="eq:eq28">\[\begin{equation}
\omega_{t} = \log y_{t}
\tag{2.28}
\end{equation}\]</span></p>
<p>However, care has to be taken when back transforming a log transformed forecast to the original scale. Intuitively, one would obtain the back transformed forecast <span class="math inline">\(\hat{y}_{t+h}\)</span> by setting it equal to <span class="math inline">\(e^{\hat{\omega}_{t+h}}\)</span>, where <span class="math inline">\(e\)</span> is Euler’s number. However, assuming that the forecast distribution of <span class="math inline">\(\omega_{t+h}\)</span>, <span class="math inline">\(\Omega_{t+h}\)</span>, is normal, with mean <span class="math inline">\(\mu_{\Omega_{t+h}}\)</span> and variance <span class="math inline">\(\sigma_{\Omega_{t+h}}^{2}\)</span>, then the forecast distribution of <span class="math inline">\(y_{t+h}\)</span>, <span class="math inline">\(Y_{t+h}\)</span>, follows a log-normal distribution, with mean <span class="math inline">\(\mu_{Y_{t+h}}\)</span> as defined in Equation <a href="2-two.html#eq:eq29">(2.29)</a>.</p>
<p><span class="math display" id="eq:eq29">\[\begin{equation}
\mu_{Y_{t+h}} = e^{(\mu_{\Omega_{t+h}} + 0.5 \sigma_{\Omega_{t+h}}^{2})}
\tag{2.29}
\end{equation}\]</span></p>
<p>For the proof of this theorem, see <span class="citation">(<strong>dambolena2009?</strong>)</span>. <span class="citation">(<strong>hyndman2018fpp?</strong>)</span> refer to <span class="math inline">\(\mu_{Y_{t+h}}\)</span> as the <em>bias-adjusted point forecast</em>.</p>
</div>
</div>
<div id="twofourthree" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Naïve forecasts</h3>
<p>It is common practice to compare the errors of forecasts obtained with a fitted time series model to those of forecasts obtained with a very simple forecasting method. Such a simple method, is in that case referred to as a <em>baseline method</em>. If the more sophisticated model does not lead to considerably better forecast accuracies than the baseline, it can be dismissed <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>.</p>
<p>One of the simplest forecast methods around, often used as a baseline, is known as the <em>naïve method</em>. When forecasting with the naïve method, all forecasted values will be equal to the last observation, no matter how far the forecasting window <span class="math inline">\(h\)</span> reaches, as shown in Equation <a href="2-two.html#eq:eq30">(2.30)</a>.</p>
<p><span class="math display" id="eq:eq30">\[\begin{equation}
\hat{y}_{t+h} = y_{t}
\tag{2.30}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted data value <span class="math inline">\(h\)</span> time lags into the future, and <span class="math inline">\(y_{t}\)</span> is the last observed data value.</p>
</div>
<div id="twofourfour" class="section level3" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Seasonal forecasts</h3>
<p>ARIMA models as described in section 2.4.2 are designed for data without a seasonal component. With some modifications, they can be applied to seasonal data as well. That works as follows. Instead of an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model, an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>)(<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>) model is fitted to the data. The (<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>) part of the model works in a similar fashion as the (<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) part, but relates to the seasonal component of the data. Hence, <span class="math inline">\(P\)</span> is the number of seasonal autoregressive terms in the model, <span class="math inline">\(D\)</span> is the order of seasonal differencing, and <span class="math inline">\(Q\)</span> is the number of seasonal moving average terms. Where <span class="math inline">\(p = 1\)</span> means that the past observation <span class="math inline">\(y_{t-1}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>, setting <span class="math inline">\(P = 1\)</span> means that the past observation <span class="math inline">\(y_{t-m}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>, with <span class="math inline">\(m\)</span> being the number of observations per seasonal cycle. The same holds for <span class="math inline">\(Q\)</span>: setting <span class="math inline">\(Q = 1\)</span>, means that the past error <span class="math inline">\(\epsilon_{t-m}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>. Regarding the seasonal differencing parameter <span class="math inline">\(D\)</span>, the first order seasonal difference of a time series is the series of changes from one seasonal period to the next <span class="citation">(<strong>shumway2011?</strong>)</span>.</p>
<p>However, ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>)(<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>), as well as several other commonly used seasonal models such as Holt Winters exponential smoothing, have two main limitations which make them unsuitable for some kind of data. Firstly, they were primarily designed to work with shorter seasonal periods, such as monthly data with patterns that repeat every year (i.e. <span class="math inline">\(m = 12\)</span>). In the case of longer seasonal periods, which may occur for example in daily and sub-daily data, modelling becomes inefficient. In the R statistical software, for example, the <code>forecast</code> package <span class="citation">(<strong>forecast?</strong>)</span> only allows seasonal periods up to <span class="math inline">\(m = 350\)</span> <span class="citation">(<strong>hyndmanblog?</strong>)</span>.</p>
<p>Secondly, these models can not handle more than one seasonal pattern at a time. Again, this can be problematic especially for daily data, in which both a weekly and yearly pattern may exist, and sub-daily data, in which even a daily, weekly and yearly pattern may exist. One of the alternative approaches proposed by <span class="citation">(<strong>hyndman2018fpp?</strong>)</span> works as follows. First, the data is decomposed into a trend, seasonal and remainder component. Then, the trend and remainder component are together modelled by a non-seasonal model, such as ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>), and forecasted accordingly. The seasonal component, in turn, can be forecasted with a seasonal naïve method, meaning that the forecasted value will be equal to the last observed value from the same season of the year. That is, the seasonal forecast for timestamp <span class="math inline">\(y_{t+h}\)</span> will be equal to the last observed value in the sequence {<span class="math inline">\(y_{t+h-m \times 1}, y_{t+h-m \times 2}, ...\)</span>}. Then, the non-seasonal and seasonal forecasts are added back together, to obtain a single forecasted value.</p>
</div>
</div>
<div id="time-series-clustering" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Time series clustering</h2>
<div id="dissimilarity-measures" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Dissimilarity measures</h3>
<p>Time series clustering is a specific domain within the field of time series analysis. Given a set of individual time series, its objective is to group similar time series into the same cluster <span class="citation">(<strong>keogh2005?</strong>)</span>. Logically, this involves the calculation of a measure that represents the similarity, or dissimilarity, between two time series. A wide range of such measures exist, some of them based directly on the observations, others on specific features of the time series, and others on parameters or residual characteristics of models fitted to the time series.</p>
<p>Whenever the time series are of the same length, and observed at the same timestamps, a simple dissimilarity measure can be calculated by summing the ordered point-to-point distances between them. A well-known distance function is the Euclidean distance, as defined in Equation <a href="2-two.html#eq:eq31">(2.31)</a> <span class="citation">(<strong>cassisi2012?</strong>)</span>.</p>
<p><span class="math display" id="eq:eq31">\[\begin{equation}
d(Y,X) = \sqrt{\sum_{t=1}^{n}(y_{t}-x_{t})^{2}}
\tag{2.31}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(d(X,Y)\)</span> is the dissimilarity value between time series <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math inline">\(y_{t}\)</span> and <span class="math inline">\(x_{t}\)</span> are the observations in respectively <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(n\)</span> is the length of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. A higher value of <span class="math inline">\(d(X,Y)\)</span> implies less similar time series. By definition, <span class="math inline">\(d(X,Y) \geq 0\)</span>.</p>
<p>Using Euclidean distance as a dissimilarity measure is simple, but has drawbacks for certain applications. For example, it can not handle time series of different length, it is sensitive to outliers, and it can not capture out-of-phase similarities, which occur when two time series show similar patterns, but shifted over time. To deal with one or more of these drawbacks, several other dissimilarity measures for time series were developed, of which the <em>dynamic time warping distance</em> is best known. Dynamic time warping is based on classical algorithms for comparing discrete sequences with continuous sequences, and basically replaces the one-to-one comparison of Euclidean distances with a many-to-one comparison. However, despite its simplicity, the Euclidean distance approach still turns out to be very competitive with the more complex methods <span class="citation">(<strong>cassisi2012?</strong>)</span>.</p>
<p>Once the dissimilarity values for all possible combinations between the analyzed time series are calculated, they are stored in an <span class="math inline">\(n \times n\)</span> matrix, where <span class="math inline">\(n\)</span> is the number of analyzed time series, and the diagonal entries equal zero. Such a matrix is commonly referred to as a dissimilarity matrix, and can be used inside conventional clustering algorithms. In time series analysis, k-means clustering and hierarchical clustering are the most popular options for this task <span class="citation">(<strong>wang2006?</strong>)</span>. The latter is used in this thesis, and its theory is summarized briefly in the next section, based on <span class="citation">(<strong>gan2007?</strong>)</span>, Chapter 7 and Chapter 17.</p>
</div>
<div id="hierarchical-clustering" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Hierarchical clustering</h3>
<p>Hierarchical clustering algorithms divide a set of data points into a sequence of nested partitions, where each partition consists of a different number of clusters. Two main types are distinguished: <em>agglomerative</em> hierarchical clustering, and <em>divisive</em> hierarchical clustering. In the first, the starting point is a partition in which all data points form a cluster on their own. Then, the two closest clusters, based on a pre-defined dissimilarity measure, are merged. This process is repeated until all data points are in one single cluster. The divisive method works the other way around: all data points start in the same cluster, which is split repeatedly, until the number of clusters equals the number of data points. Agglomerative hierarchical clustering is the most popular of the two types, and forms the focus of this section.</p>
<p>Again, defining a suitable dissimilarity measure is a core task in the process. A simple example is the <em>single link method</em>, where the dissimilarity between two clusters is defined as the shortest distance between a data point in the first cluster and a data point in the second cluster. In contradiction, the <em>complete link method</em> calculates the dissimilarity as being the longest possible distance between them. Other methods include the <em>group average method</em>, which calculates the average of the shortest distances between all pairs of data points, and the <em>centroid method</em>, which calculates the shortest distance between the cluster centroids. In all cases, the Euclidean distance is commonly used as the distance function.</p>
<p>A more general approach is known as the <em>Ward method</em>, developed by <span class="citation">(<strong>ward1963?</strong>)</span>. In this method, the dissimilarity between two clusters is defined as the loss of information when the clusters are merged. The information loss of a merge between two clusters is quantified with Equation <a href="2-two.html#eq:eq32">(2.32)</a>.</p>
<p><span class="math display" id="eq:eq32">\[\begin{equation}
\Delta I = I(C_{m}) - I(C_{1}) - I(C_{2})
\tag{2.32}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\Delta I\)</span> is the information loss, <span class="math inline">\(C_{m}\)</span> is the merge between clusters <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>, and <span class="math inline">\(I(C_{m})\)</span>, <span class="math inline">\(I(C_{1})\)</span> and <span class="math inline">\(I(C_{2})\)</span> are the information criteria of respectively <span class="math inline">\(C_{m}\)</span>, <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>. <span class="citation">(<strong>ward1963?</strong>)</span> did not put a hard restriction on how such an information criterion should be quantified, but usually, it is set to be the <em>error sum of squares</em>, calculated with Equation <a href="2-two.html#eq:eq33">(2.33)</a>.</p>
<p><span class="math display" id="eq:eq33">\[\begin{equation}
I(C) = \sum_{i = 1}^{n} (c_{i} - \mu(C))^{2}
\tag{2.33}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(c_{i}\)</span> are the data points in <span class="math inline">\(C\)</span>, <span class="math inline">\(\mu(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, and <span class="math inline">\(n\)</span> is the number of data points in <span class="math inline">\(C\)</span>. In other words, <span class="math inline">\(I(C)\)</span> is the sum of the Euclidean distances from all data points in the cluster, to the center of mass of the cluster.</p>
<p>If one is interested in obtaining only one single partition, it is necessary to find a suitable value for the desired number of clusters <span class="math inline">\(k\)</span>. It is common practice to do so by visually interpreting the dendrogram of the hierarchical clustering, which is a diagram representing the output in a tree structure, but automated approaches have been developed as well. Most of them are based on the idea that in an ideal situation, clusters are compact and clearly separated from each other. However, minimizing the variance within the clusters will always favor the situation where each data point forms a cluster on its own, while maximizing the variance between the clusters will always favor the situation where all data points are together in one cluster. Therefore, most approaches combine those two operations, in order to find the best possible partition.</p>
<p>An example of such an approach is the <em>Dunn Index</em>, developed by <span class="citation">(<strong>dunn1974?</strong>)</span>. For a specific partition into <span class="math inline">\(k\)</span> clusters, it calculates the ratio of the smallest distance between two data points that are not in the same cluster to the largest distance between two data points that are in the same cluster. This is shown in Equation <a href="2-two.html#eq:eq34">(2.34)</a>.</p>
<p><span class="math display" id="eq:eq34">\[\begin{equation}
V(\Lambda_{k}) = \min \Bigg\{ \min \Bigg(\frac{D(C_{i}, C_{j})}{\max diam(C_{l})} \Bigg) \Bigg\}
\tag{2.34}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(V(\Lambda_{k})\)</span> is the Dunn Index of a partition <span class="math inline">\(\Lambda_{k}\)</span> with <span class="math inline">\(k\)</span> clusters, <span class="math inline">\(1 \leq i, j, l \leq k\)</span>, <span class="math inline">\(D(C_{i}, C_{j})\)</span> is the Euclidean distance between a data point in cluster <span class="math inline">\(C_{i} \in \Lambda_{k}\)</span> and a data point in cluster <span class="math inline">\(C_{j} \in \Lambda_{k}\)</span>, given that <span class="math inline">\(C_{i} \neq C_{j}\)</span>, and <span class="math inline">\(diam(C_{l})\)</span> is the largest Euclidean distance between two data points in cluster <span class="math inline">\(C_{l} \in \Lambda_{k}\)</span>. To find the optimal partition <span class="math inline">\(\Lambda_{k}^*\)</span>, the Dunn Index should be maximized.</p>
</div>
<div id="twofivethree" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Spatial time series clustering</h3>
<p>Spatial time series are time series with a spatial reference, i.e. time series that are linked to geographical locations <span class="citation">(<strong>kamarianakis2005?</strong>)</span>. With such series, similarity can not only be expressed in terms of similar data values, but also in terms of spatial proximity. These two, however, are likely to be related, given the concept of spatial dependence, which is similar to the temporal autocorrelation described in Section <a href="2-two.html#twotwoone">2.2.1</a>, in the sense that the structure of the correlation between random variables is derived from a specific ordering, determined by their relative position in geographic space <span class="citation">(<strong>anselin2010?</strong>)</span>. Hence, time series linked to geographical locations that are close to each other, are likely to show similar temporal patterns.</p>
<p>When clustering a set of spatial time series, it may be desired that the clusters are not only similar in data values, but also form spatially connected sets. In that case, constraints need to be imposed on the possible outcomes of the clustering process. This can can be done in a strict way, where the resulting partition is forced to consist of spatially contiguous clusters. When the spatial dependence between the series is really strong, this may be a sensible approach. In less coherent cases, however, this may group time series with different patterns into the same cluster, just because they are close to each other in space. Hence, an adequate balance between the data similarity and the spatial similarity, needs to be found, without artificially forcing a strong spatial dependence on the time series.</p>
<p>For this, <span class="citation">(<strong>clustgeo?</strong>)</span> developed a variation on the hierarchical clustering algorithm, called <em>spatially constrained hierarchical clustering</em>, which is summarized in this section. The algorithm takes two dissimilarity matrices as input. The first one gives the dissimilarity values in the <em>feature space</em>, i.e. the dissimilarities of the data values of the observations, while the latter gives the dissimilarity values in the <em>constraint space</em>, i.e. the dissimilarities of the geographical locations of the observations.</p>
<p>Spatially constrained hierarchical clustering uses a Ward-like method to define which clusters will be merged at each step, but with a different definition of the information criterion of a cluster, as shown in Equation <a href="2-two.html#eq:eq35">(2.35)</a>.</p>
<p><span class="math display" id="eq:eq35">\[\begin{equation}
I(C) =
(1-\alpha)\sum_{i=1}^{n}(c_{i} - \mu(C))^{2} +
\alpha\sum_{i=1}^{n}(c_{i}^{*} - \mu^{*}(C))^{2}
\tag{2.35}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(c_{i}\)</span> are the data points in <span class="math inline">\(C\)</span>, with normalized values taken from the feature space dissimilarity matrix, and <span class="math inline">\(\mu(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, computed with those values. <span class="math inline">\(c_{i}^{*}\)</span> are the same data points, but with normalized values taken from the constraint space dissimilarity matrix, and <span class="math inline">\(\mu^{*}(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, computed with those values. Furthermore, <span class="math inline">\(n\)</span> is the number of data points in <span class="math inline">\(C\)</span> and <span class="math inline">\(\alpha\)</span> is the mixing parameter, where <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. <span class="citation">(<strong>clustgeo?</strong>)</span> also present more general approaches, in which the calculated distances in Equation <a href="2-two.html#eq:eq35">(2.35)</a> are not necessarily Euclidean, and where observations can be weighted, but these are not covered in this thesis.</p>
<p>With the information criterion of a single cluster calculated as in Equation <a href="2-two.html#eq:eq35">(2.35)</a>, the information loss of a merge between two clusters is calculated in the same way as in regular Ward hierarchical clustering (Equation <a href="2-two.html#eq:eq33">(2.33)</a>). Then, at each merging step, two clusters are merged such that the loss of information is minimized.</p>
<p>The mixing parameter <span class="math inline">\(\alpha\)</span> plays a key role in spatially constrained hierarchical clustering. It sets the importance that is given to the spatial constraint. The higher <span class="math inline">\(\alpha\)</span>, the more the result of the clustering procedure is influenced by the spatial locations of the data points. When <span class="math inline">\(\alpha = 0\)</span>, the data are clustered without any spatial constraint, while <span class="math inline">\(\alpha = 1\)</span> leads to a clustering only based on the spatial location of the data points. Therefore, it is important to determine a suitable value for <span class="math inline">\(\alpha\)</span>. <span class="citation">(<strong>clustgeo?</strong>)</span> propose the following approach. First, <span class="math inline">\(\alpha\)</span> is set to zero, and a hierarchical clustering without spatial constraint is performed. The resulting sequence of partitions is rendered as a dendrogram, and the optimal number of clusters <span class="math inline">\(k^{*}\)</span> is defined visually. Then, several spatially constrained hierarchical clustering procedures are performed, each with a different value of <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(k = k^{*}\)</span>. Since <span class="math inline">\(k\)</span> is fixed, the outputs are always single partitions. For each cluster in such a partition the information criterion regarding the feature data (i.e. the first part of Equation <a href="2-two.html#eq:eq35">(2.35)</a>) and the information criterion regarding the constraint data (i.e. the second part of Equation <a href="2-two.html#eq:eq35">(2.35)</a>), are calculated separately, and also summed separately over all clusters in the partition. These two summed values are then plotted, with <span class="math inline">\(\alpha\)</span> on the x-axis. In the end, this lead to a plot that shows the loss of information in the feature space and the gain of information in the constraint space, as <span class="math inline">\(\alpha\)</span> gets larger. With such a plot, <span class="math inline">\(\alpha\)</span> can be chosen such that the trade-off between the loss and gain of information in the two spaces is considered acceptable.</p>
<!-- \clearpage -->
<!-- \shipout\null -->
<!-- \stepcounter{page} -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-three.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
