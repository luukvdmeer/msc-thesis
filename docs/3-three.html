<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 System architecture | thesis.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="3 System architecture | thesis.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 System architecture | thesis.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-two.html"/>
<link rel="next" href="4-four.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><i class="fa fa-check"></i>Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</a>
<ul>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#keywords"><i class="fa fa-check"></i>Keywords</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html"><a href="spatio-temporal-forecasts-for-bike-availability-in-dockless-bike-sharing-systems.html#abbreviations"><i class="fa fa-check"></i>Abbreviations</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-introduction.html"><a href="1-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-introduction.html"><a href="1-introduction.html#context"><i class="fa fa-check"></i><b>1.1</b> Context</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction.html"><a href="1-introduction.html#objective"><i class="fa fa-check"></i><b>1.2</b> Objective</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction.html"><a href="1-introduction.html#onethree"><i class="fa fa-check"></i><b>1.3</b> Related work</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-introduction.html"><a href="1-introduction.html#onethreeone"><i class="fa fa-check"></i><b>1.3.1</b> Forecasting in station-based systems</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-introduction.html"><a href="1-introduction.html#onethreetwo"><i class="fa fa-check"></i><b>1.3.2</b> Forecasting in dockless systems</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-introduction.html"><a href="1-introduction.html#approach"><i class="fa fa-check"></i><b>1.4</b> Approach</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction.html"><a href="1-introduction.html#outline"><i class="fa fa-check"></i><b>1.5</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-two.html"><a href="2-two.html"><i class="fa fa-check"></i><b>2</b> Theoretical background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-two.html"><a href="2-two.html#time-series-definition"><i class="fa fa-check"></i><b>2.1</b> Time series definition</a></li>
<li class="chapter" data-level="2.2" data-path="2-two.html"><a href="2-two.html#time-series-characteristics"><i class="fa fa-check"></i><b>2.2</b> Time series characteristics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-two.html"><a href="2-two.html#twotwoone"><i class="fa fa-check"></i><b>2.2.1</b> Autocorrelation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-two.html"><a href="2-two.html#stationarity"><i class="fa fa-check"></i><b>2.2.2</b> Stationarity</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-two.html"><a href="2-two.html#twotwothree"><i class="fa fa-check"></i><b>2.2.3</b> Spectral entropy</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-two.html"><a href="2-two.html#time-series-components"><i class="fa fa-check"></i><b>2.3</b> Time series components</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-two.html"><a href="2-two.html#definitions"><i class="fa fa-check"></i><b>2.3.1</b> Definitions</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-two.html"><a href="2-two.html#classical-decomposition"><i class="fa fa-check"></i><b>2.3.2</b> Classical decomposition</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-two.html"><a href="2-two.html#twothreethree"><i class="fa fa-check"></i><b>2.3.3</b> STL decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-two.html"><a href="2-two.html#twofour"><i class="fa fa-check"></i><b>2.4</b> Time series forecasting</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-two.html"><a href="2-two.html#twofourone"><i class="fa fa-check"></i><b>2.4.1</b> Forecasting models</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-two.html"><a href="2-two.html#arima"><i class="fa fa-check"></i><b>2.4.2</b> ARIMA</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-two.html"><a href="2-two.html#twofourthree"><i class="fa fa-check"></i><b>2.4.3</b> Naïve forecasts</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-two.html"><a href="2-two.html#twofourfour"><i class="fa fa-check"></i><b>2.4.4</b> Seasonal forecasts</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-two.html"><a href="2-two.html#time-series-clustering"><i class="fa fa-check"></i><b>2.5</b> Time series clustering</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="2-two.html"><a href="2-two.html#dissimilarity-measures"><i class="fa fa-check"></i><b>2.5.1</b> Dissimilarity measures</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-two.html"><a href="2-two.html#hierarchical-clustering"><i class="fa fa-check"></i><b>2.5.2</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-two.html"><a href="2-two.html#twofivethree"><i class="fa fa-check"></i><b>2.5.3</b> Spatial time series clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-three.html"><a href="3-three.html"><i class="fa fa-check"></i><b>3</b> System architecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-three.html"><a href="3-three.html#overall-design"><i class="fa fa-check"></i><b>3.1</b> Overall design</a></li>
<li class="chapter" data-level="3.2" data-path="3-three.html"><a href="3-three.html#threetwo"><i class="fa fa-check"></i><b>3.2</b> Software</a></li>
<li class="chapter" data-level="3.3" data-path="3-three.html"><a href="3-three.html#threethree"><i class="fa fa-check"></i><b>3.3</b> System area</a></li>
<li class="chapter" data-level="3.4" data-path="3-three.html"><a href="3-three.html#threefour"><i class="fa fa-check"></i><b>3.4</b> Database</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-three.html"><a href="3-three.html#threefourone"><i class="fa fa-check"></i><b>3.4.1</b> Distance data</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-three.html"><a href="3-three.html#threefourtwo"><i class="fa fa-check"></i><b>3.4.2</b> Usage data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-three.html"><a href="3-three.html#threefive"><i class="fa fa-check"></i><b>3.5</b> Forecast request</a></li>
<li class="chapter" data-level="3.6" data-path="3-three.html"><a href="3-three.html#threesix"><i class="fa fa-check"></i><b>3.6</b> Cluster loop</a></li>
<li class="chapter" data-level="3.7" data-path="3-three.html"><a href="3-three.html#threeseven"><i class="fa fa-check"></i><b>3.7</b> Model loop</a></li>
<li class="chapter" data-level="3.8" data-path="3-three.html"><a href="3-three.html#threeeight"><i class="fa fa-check"></i><b>3.8</b> Forecast loop</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-four.html"><a href="4-four.html"><i class="fa fa-check"></i><b>4</b> Data and experimental design</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-four.html"><a href="4-four.html#fourone"><i class="fa fa-check"></i><b>4.1</b> Data source</a></li>
<li class="chapter" data-level="4.2" data-path="4-four.html"><a href="4-four.html#data-retrieval"><i class="fa fa-check"></i><b>4.2</b> Data retrieval</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-four.html"><a href="4-four.html#distance-data"><i class="fa fa-check"></i><b>4.2.1</b> Distance data</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-four.html"><a href="4-four.html#usage-data"><i class="fa fa-check"></i><b>4.2.2</b> Usage data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-four.html"><a href="4-four.html#experimental-design"><i class="fa fa-check"></i><b>4.3</b> Experimental design</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-four.html"><a href="4-four.html#training-and-test-periods"><i class="fa fa-check"></i><b>4.3.1</b> Training and test periods</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-four.html"><a href="4-four.html#additional-software"><i class="fa fa-check"></i><b>4.3.2</b> Additional software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-five.html"><a href="5-five.html"><i class="fa fa-check"></i><b>5</b> Results and discussion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-five.html"><a href="5-five.html#clustering"><i class="fa fa-check"></i><b>5.1</b> Clustering</a></li>
<li class="chapter" data-level="5.2" data-path="5-five.html"><a href="5-five.html#model-building"><i class="fa fa-check"></i><b>5.2</b> Model building</a></li>
<li class="chapter" data-level="5.3" data-path="5-five.html"><a href="5-five.html#forecasting-1"><i class="fa fa-check"></i><b>5.3</b> Forecasting</a></li>
<li class="chapter" data-level="5.4" data-path="5-five.html"><a href="5-five.html#limitations-and-recommendations"><i class="fa fa-check"></i><b>5.4</b> Limitations and recommendations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="5-five.html"><a href="5-five.html#limits-of-forecastability"><i class="fa fa-check"></i><b>5.4.1</b> Limits of forecastability</a></li>
<li class="chapter" data-level="5.4.2" data-path="5-five.html"><a href="5-five.html#exogenous-variables"><i class="fa fa-check"></i><b>5.4.2</b> Exogenous variables</a></li>
<li class="chapter" data-level="5.4.3" data-path="5-five.html"><a href="5-five.html#residual-distributions-and-prediction-intervals"><i class="fa fa-check"></i><b>5.4.3</b> Residual distributions and prediction intervals</a></li>
<li class="chapter" data-level="5.4.4" data-path="5-five.html"><a href="5-five.html#gps-accuracy"><i class="fa fa-check"></i><b>5.4.4</b> GPS accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-six.html"><a href="6-six.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-aa.html"><a href="A-aa.html"><i class="fa fa-check"></i><b>A</b> Code</a></li>
<li class="chapter" data-level="B" data-path="B-ab.html"><a href="B-ab.html"><i class="fa fa-check"></i><b>B</b> Models</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-ab.html"><a href="B-ab.html#bayview-model-point"><i class="fa fa-check"></i><b>B.1</b> Bayview model point</a></li>
<li class="chapter" data-level="B.2" data-path="B-ab.html"><a href="B-ab.html#downtown-model-point"><i class="fa fa-check"></i><b>B.2</b> Downtown model point</a></li>
<li class="chapter" data-level="B.3" data-path="B-ab.html"><a href="B-ab.html#residential-model-point"><i class="fa fa-check"></i><b>B.3</b> Residential model point</a></li>
<li class="chapter" data-level="B.4" data-path="B-ab.html"><a href="B-ab.html#presidio-model-point"><i class="fa fa-check"></i><b>B.4</b> Presidio model point</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="three" class="section level1" number="3">
<h1><span class="header-section-number">3</span> System architecture</h1>
<p>This chapter describes the methodology of DBAFS. It builds on the theory discussed in Chapter <a href="2-two.html#two">2</a>, and is structured as follows. In the first section, a general overview of the complete forecasting system is given. Section two presents the software that underlies DBAFS. The third, fourth and fifth section discusses the inputs to the system, including the computations that are done on the database server of the dockless bike sharing system. Subsequently, section six, seven and eight cover the detailed methodologies of all the distinct components of the system architecture separately.</p>
<div id="overall-design" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Overall design</h2>
<p>The goal of DBAFS is to forecast the distance to the nearest available bike for a given location and a given timestamp in the future. It is meant to be used by both the operators and users of a dockless bike sharing system, which from now on are referred to as <em>users</em> of DBAFS. A forecast is made every time a user requests one. In intensively used bike sharing systems, this can mean that several hundreds of forecasts are required every day, all based on different historical datasets. All these datasets usually consist of a time series with a high temporal resolution. Although the data may be complex, it would be inconvenient for the users if forecasts take a lot of time or need manual interventions. Taking into consideration the above-mentioned challenges, DBAFS should be a <em>fast</em> and <em>automated</em> process that still produces as <em>accurate</em> forecasts as possible. Optimizing all of them, is a utopia. Faster forecasts, will probably have a negative affect on the accuracy, and aiming for an automated procedure, means that models can not be manually updated and fine-tuned. The goal, therefore, is to find an acceptable compromise between the three requirements.</p>
<p>The most time consuming part of the system is the selection of an appropriate model and the estimation of its parameters. If this had to be done at every forecast request separately, forecasts would take too much time. Therefore, in DBAFS, forecasting models are build only once in a while at a limited number of locations. Each individual forecast will inherit the structure and parameters of one of those pre-build models, rather than building a new model on its own.</p>
<p>The approach of building models only at a limited number of locations, involves the selection of those locations. In DBAFS, this is done by dividing the system area of the dockless bike sharing system into spatially contiguous clusters, where each cluster contains the areas that show similar weekly patterns in the historical data. Then, each cluster is represented by a single <em>model point</em>, which is a geographical location where a model is build. An individual forecast takes the model structure and parameters of the model point that is in the same cluster as the location of the forecast.</p>
<p>The architecture of DBAFS builds on two main assumptions, one regarding the spatial dependence in the data, and the other regarding the temporal dependence in the data. Firstly, it is assumed that the processes generating the historical data are spatially dependent, such that spatially contiguous clusters containing areas with similar temporal patterns can be constructed, and moreover, that they are similar enough at each location in a cluster to be described by the same model. Secondly, it is assumed that these processes do not change radically over a short time period, such that a model fitted to a set of historical data, can still adequately describe new data coming from a location in the same cluster. Of course, these assumptions will not always be completely valid, but are made to obtain a reasonable compromise between fast and accurate forecasts.</p>
<p>The clustering, model building and forecasting processes can be seen as three distinct processing loops, that together make up DBAFS. The forecast loop runs every time a user makes a forecast request. The model loop only runs every <span class="math inline">\(n_{m}\)</span> weeks, and the cluster loop every <span class="math inline">\(n_{c}\)</span> weeks. <span class="math inline">\(n_{m}\)</span> should be chosen such that new models are build when the patterns in the historical data have changed considerably. <span class="math inline">\(n_{c}\)</span> should be chosen such that new clusters are defined when the spatial distribution of the weekly patterns in the historical data has changed considerably, and will, in most cases, be much larger than <span class="math inline">\(n_{m}\)</span>. The cluster, model and forecast loops are all completely automated and do not require any manual interventions. The overall design of DBAFS as described above is summarized in Figure <a href="3-three.html#fig:overalldesign">3.1</a>. The inputs of the system, i.e. the system area, database and forecast request, are covered in Section <a href="3-three.html#threethree">3.3</a>, <a href="3-three.html#threefour">3.4</a> and <a href="3-three.html#threefive">3.5</a>, respectively, while Section <a href="3-three.html#threesix">3.6</a>, <a href="3-three.html#threeseven">3.7</a> and <a href="3-three.html#threeeight">3.8</a> describe the detailed designs of the three processing loops.</p>
<div class="figure" style="text-align: center"><span id="fig:overalldesign"></span>
<img src="figures/workflow.png" alt="Overall design of DBAFS" width="\textwidth" />
<p class="caption">
Figure 3.1: Overall design of DBAFS
</p>
</div>
</div>
<div id="threetwo" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Software</h2>
<p>The underlying code of DBAFS (see Appendix <a href="A-aa.html#aa">A</a>) is written in the R programming language <span class="citation">(<strong>rlanguage?</strong>)</span>. However, Structured Query Language (SQL) statements are nested within the R code to retrieve data from a PostgreSQL database <span class="citation">(<strong>postgres?</strong>)</span>, and to run some of the heavier data pre-processing computations on the database server. These computations are discussed in Section <a href="3-three.html#threefour">3.4</a>.</p>
<p>On top of functions that are included in R by default, DBAFS makes use of several extensions, listed below.</p>
<ul>
<li>The <code>ClustGeo</code> package, for spatially constrained clustering <span class="citation">(<strong>clustgeo?</strong>)</span>.</li>
<li>The <code>clValid</code> package, for calculating the Dunn Index <span class="citation">(<strong>clValid?</strong>)</span>.</li>
<li>The <code>forecast</code> package, for building forecasting models, decomposing time series, and forecasting time series <span class="citation">(<strong>forecast?</strong>)</span>.</li>
<li>The <code>lubridate</code> package, for processing dates and timestamps <span class="citation">(<strong>lubridate?</strong>)</span>.</li>
<li>The <code>RPostgreSQL</code> package, for connecting to a PostgreSQL database and running SQL code on the database server <span class="citation">(<strong>RPostgreSQL?</strong>)</span>.</li>
<li>The <code>sf</code> package, for processing spatial data <span class="citation">(<strong>sf?</strong>)</span>.</li>
<li>The <code>tsibble</code> package, for pre-processing time series datasets <span class="citation">(<strong>tsibble?</strong>)</span>.</li>
</ul>
</div>
<div id="threethree" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> System area</h2>
<p>Each dockless bike sharing system has a system area, in which the bikes can be used. Usually, leaving a bike outside of the system area, will result in a fine. DBAFS produces forecasts only inside the system area, and therefore, the geographical outline of this area needs to be provided. DBAFS accepts all filetypes that can be read with a driver supported by the <code>st_read</code> function in the <code>sf</code> package, given that the included feature is either a polygon or multipolygon. The accepted filetypes include, among others, ESRI shapefiles, GeoPackage files and GeoJSON files. It is also possible to retrieve the feature from a PostgreSQL database.</p>
</div>
<div id="threefour" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Database</h2>
<p>In a dockless bike sharing system, each bike is equipped with a Global Positioning System (GPS). Every <span class="math inline">\(i_{d}\)</span> minutes, the geographical locations of all bikes are saved into a database, together with the corresponding timestamp. The locations of the bikes that are not in use at the current time, and thus available, are visible to the users of the system in a mobile application, and stored separately from the data regarding bikes that are in use.</p>
<p>The geographical location of a bike is spatial data, and should be stored as such. An advanced and open source database management system for spatial data is PostgreSQL in combination with the PostGIS extension. DBAFS requires the data to be stored in such a database, and to have a sub-daily temporal resolution. Each feature represents the location of an available bike at a certain timestamp and should at least have the following fields.</p>
<ul>
<li>A timestamp of data type <code>timestamp with time zone</code>.</li>
<li>A geographical location of data type <code>geometry(Point)</code>.</li>
<li>A unique ID of the bike to which the feature belongs.</li>
</ul>
<p>Data are pre-processed on the database server, and only the data that are needed, are loaded into memory. In DBAFS, this pre-processing step involves two different procedures. The first one leads to data that contain information about the distance to the nearest bike for several timestamps in the past, and is discussed in the next section, while the latter produces a dataset with all the bicycle pick-ups in the database, and is discussed in Section <a href="3-three.html#threefourtwo">3.4.2</a>.</p>
<div id="threefourone" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Distance data</h3>
<p>For a given location, the distance from that location to the nearest available bike is calculated for each timestamp <span class="math inline">\(t \in T\)</span>, where <span class="math inline">\(T\)</span> is a regularly spaced time interval containing timestamps within the timespan of the historical data. The temporal resolution of <span class="math inline">\(T\)</span> equals <span class="math inline">\(i_{s}\)</span> minutes, where <span class="math inline">\(i_{s} \geq i_{d}\)</span>. The nearest available bike is found by a nearest neighbour searching process that uses spatial indices on the geometries. In practice, this means that it is not needed to first compute the distances to all available bikes, which would slow down the process vastly. If no bike can be found, for example due to a server error at that timestamp, or the unlikely event that there are no bikes available anywhere in the system, the corresponding feature will be inserted in the data, with a non-available distance value, <span class="math inline">\(NA\)</span>. That is, after pre-processing, the resulting time series will always be regular, with all timestamps <span class="math inline">\(t \in T\)</span> present. This also means that when data are queried for several locations at the same times, the resulting time series will always have the same length.</p>
<p>The calculated distances are great-circle distances assuming a spherical earth with a radius equal to the mean radius of the WGS84 ellipsoid, as showed in Equation <a href="3-three.html#eq:eq36">(3.1)</a>.</p>
<p><span class="math display" id="eq:eq36">\[\begin{equation}
L_{AB} = \frac{(2a+b)}{3} \times
\frac{\pi}{180} \times
arccos(sin\phi_{A}sin\phi_{B}+cos\phi_{A}cos\phi_{B}cos\Delta\lambda)
\tag{3.1}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(L_{AB}\)</span> is the great-circle distance between point <span class="math inline">\(A\)</span> and point <span class="math inline">\(B\)</span> in meters, <span class="math inline">\(\phi_{A}\)</span> and <span class="math inline">\(\phi_{B}\)</span> are the latitudes of respectively point <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in degrees on the WGS84 ellipsoid, and <span class="math inline">\(\Delta\lambda\)</span> is the difference in longitude between the two points, i.e. <span class="math inline">\(\lambda_{B}-\lambda_{A}\)</span>, in degrees on the WGS84 ellipsoid. Furthermore, <span class="math inline">\(a\)</span> is the equatorial radius of the WGS84 ellipsoid in meters, which is defined to be <span class="math inline">\(6378137\)</span>, and <span class="math inline">\(b\)</span> is the polar radius of the WGS84 ellipsoid in meters, which is defined to be <span class="math inline">\(6378137 \times (1 - 298.257 223 563^{-1}) = 6 356 752.3142\)</span> <span class="citation">(<strong>iliffe2008?</strong>)</span>.</p>
<p>The sphere is chosen since calculating distances on the ellipsoid itself slows down computations, and, on the geographical scale of a dockless bike sharing system, has an accuracy gain that can be neglected. Working with the shortest distance over the street network might in most cases be more appropriate, but at the same time involves much more complex computations, especially when either the given location or the locations of the bikes are not exactly on the network lines.</p>
<p>The output of this pre-processing operation is a time series with <span class="math inline">\(T\)</span> features and a temporal resolution of <span class="math inline">\(i_{s}\)</span>, belonging to one single location in the system area of the dockless bike sharing system. Each feature contains a timestamp and the great-circle distance from the given location to the nearest available bike in meters. Such data are referred to in this thesis as <em>distance data</em>.</p>
</div>
<div id="threefourtwo" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Usage data</h3>
<p>A pick-up is the moment that a user of the dockless bike sharing system unlocks a bike to make a trip. For the historical database containing the locations of the available bikes, this means that the bike that is picked-up will be present in the data at the last timestamp before the pick-up, but missing at the first timestamp after the pick-up. In DBAFS, this is used to retrieve all the pick-ups from the database. Historical data with the highest possible temporal resolution, i.e. <span class="math inline">\(i_{d}\)</span> minutes, are queried for one single bike ID. Then, all timestamps that are missing, are added to the data, but without an available location. If feature <span class="math inline">\(j\)</span> has an available location, but feature <span class="math inline">\(j+1\)</span> has not, <span class="math inline">\(j\)</span> is considered a pick-up. This procedure is repeated for all individual bikes. If more than 20% of the features within the same minute are considered pick-ups, it is assumed that this was caused by a server error, and they are removed from the data.</p>
<p>The output of this pre-processing operation is a data frame with all the features in the database that are considered pick-ups. Each feature has at least a timestamp, a geographical location and a bike ID. The number of pick-ups in an area represents the usage intensity of the bike sharing system. Such data are therefore referred to in this thesis as <em>usage data</em>.</p>
<p>Obviously, the procedure described in this section has some deficiencies. The removal of a bike by the system operator, for redistribution or maintenance purposes, is falsely considered to be a pick-up. Specific information about redistribution patterns can be added, but will in many cases be unavailable, and even if available, those patterns may be too irregular to implement adequately in the workflow. However, in DBAFS, the usage data are only used to define the location of the model point in a cluster, and not to analyze usage patterns into detail. Therefore, fully accurate data are not indispensable for this purpose, and the current procedure forms a sufficient basis.</p>
</div>
</div>
<div id="threefive" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Forecast request</h2>
<p>A forecast request is made by a user. DBAFS assumes such a request to be composed of the geographical coordinates of the location at which the forecast should be made. The coordinates can be expressed in any coordinate reference system that is included in the PROJ library <span class="citation">(<strong>proj?</strong>)</span>. The timestamp can be expressed in any time zone that is included in the Time Zone database <span class="citation">(<strong>tz?</strong>)</span>.</p>
</div>
<div id="threesix" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Cluster loop</h2>
<p>The main purpose of the cluster loop is to find suitable locations for the model points. The loop starts by laying a grid with square cells of <span class="math inline">\(p \times p\)</span> meters over the system area of the dockless bike sharing system, such that each location in the system area is part of one of those grid cells. Then, the geographical coordinates of the centroids of the grid cells are calculated, and <span class="math inline">\(m_{c}\)</span> weeks of distance data are queried for each of those centroids.</p>
<p>The result of this query operation is a set of <span class="math inline">\(n\)</span> time series, where <span class="math inline">\(n\)</span> is the number of cells in the overlaying grid. To reduce the dimensionality of the clustering task, each of those time series is simplified by averaging its values per hour of the week. This is followed by a min-max normalization, such that time series that show the same patterns over time, but with different means, will be considered similar. The normalized values are calculated with Equation <a href="3-three.html#eq:eq37">(3.2)</a>.</p>
<p><span class="math display" id="eq:eq37">\[\begin{equation}
\hat{y_{t}} = \frac{y_{t} - y_{min}}{y_{max} - y_{min}}
\tag{3.2}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\hat{y_{t}}\)</span> is the normalized value of <span class="math inline">\(y_{t}\)</span>, <span class="math inline">\(y_{min}\)</span> is the minimum value in the time series, and <span class="math inline">\(y_{max}\)</span> is the maximum value in the time series. By definition, <span class="math inline">\(0 \leq \hat{y_{t}} \leq 1\)</span>.</p>
<p>For all possible combinations of the <span class="math inline">\(n\)</span> averaged, normalized time series, a dissimilarity value is calculated based on the Euclidean distance between the two series, as defined in Equation <a href="2-two.html#eq:eq31">(2.31)</a>. Since all time series have the same length, and observations at the same timestamps, the Euclidean approach is appropriate, and for the sake of simplicity, chosen over dynamic time warping. Furthermore, since out-of-phase similarities are ignored, areas where similar peaks and valleys in the data occur at different times of the week, will be grouped into different clusters, which gives a better representation of the spatio-temporal dynamics of the bike sharing system.</p>
<p>All Euclidean dissimilarity values are stored together in a <span class="math inline">\(n \times n\)</span> matrix and form the time series dissimilarity matrix <span class="math inline">\(A\)</span>. At the same time, a spatial dissimilarity matrix <span class="math inline">\(B\)</span> is created. This matrix is equal to <span class="math inline">\(1-C\)</span>, where <span class="math inline">\(C\)</span> is the adjacency matrix of the <span class="math inline">\(n\)</span> grid cells. That is, <span class="math inline">\(B\)</span> is a <span class="math inline">\(n \times n\)</span> matrix in which <span class="math inline">\(b_{i,j} = 0\)</span> when grid cells <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are neighbours, and <span class="math inline">\(b_{i,j} = 1\)</span> otherwise.</p>
<p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are used as the dissimilarity matrices of respectively the feature space and the constraint space in a spatially constrained hierarchical clustering procedure, which was introduced in Section <a href="2-two.html#twofivethree">2.5.3</a>. Before the final clustering procedure can start, the number of clusters <span class="math inline">\(k\)</span> and the value of the mixing parameter <span class="math inline">\(\alpha\)</span> need to be set. DBAFS does this based on the approach proposed by <span class="citation">(<strong>clustgeo?</strong>)</span>, which was discussed in Section <a href="2-two.html#twofivethree">2.5.3</a>, but replaces the manual interpretation of plots by a fully automated method, as described below.</p>
<p>At first, only the dissimilarity values in the feature space are clustered, i.e. a spatially constrained hierarchical clustering with <span class="math inline">\(\alpha = 0\)</span> is performed, which results in a sequence of partitions {<span class="math inline">\(\Lambda_{k}\)</span>}. For each <span class="math inline">\(k \in K\)</span>, where <span class="math inline">\(K\)</span> is a finite set of strictly positive integers, the Dunn Index <span class="math inline">\(V(\Lambda_{k})\)</span> of a specific partition <span class="math inline">\(\Lambda_{k}\)</span> is calculated with Equation <a href="2-two.html#eq:eq34">(2.34)</a>. Then, the value of <span class="math inline">\(k\)</span> that maximizes <span class="math inline">\(V(\Lambda_{k})\)</span> is chosen as optimal value of <span class="math inline">\(k\)</span>, and referred to as <span class="math inline">\(k^{*}\)</span>.</p>
<p>Secondly, for each <span class="math inline">\(\omega \in \Omega\)</span>, where <span class="math inline">\(\Omega =\)</span> {<span class="math inline">\(0, 0.1, 0.2, ..., 1\)</span>}, a spatially constrained hierarchical clustering with <span class="math inline">\(k = k^{*}\)</span> and <span class="math inline">\(\alpha = \omega\)</span> is performed, which results in a set of partitions {<span class="math inline">\(\Lambda_{\omega}\)</span>}, of the same length as <span class="math inline">\(\Omega\)</span>. For each partition <span class="math inline">\(\Lambda_{\omega}\)</span>, the sum <span class="math inline">\(\sum I_{f}(C_{i}^{\omega})\)</span> and the sum <span class="math inline">\(\sum I_{c}(C_{i}^{\omega})\)</span> are calculated, where <span class="math inline">\(C_{i}^{\omega}\)</span> are the clusters in <span class="math inline">\(\Lambda_{\omega}\)</span>, <span class="math inline">\(I_{f}\)</span> is the information criterion regarding the feature data (i.e. the first part of Equation <a href="2-two.html#eq:eq35">(2.35)</a>) and <span class="math inline">\(I_{c}\)</span> is the information criterion regarding the constraint data (i.e. the second part of Equation <a href="2-two.html#eq:eq35">(2.35)</a>). Then, the value of <span class="math inline">\(\omega\)</span> that maximizes <span class="math inline">\(\sum I_{c}(C_{i}^{\omega})\)</span>, given that <span class="math inline">\(\big((\sum I_{f}(C_{i}^{\omega}) / \sum I_{f}(C_{i}^{0})\big) \geq 0.9\)</span>, is chosen as the optimal value of <span class="math inline">\(\alpha\)</span>, and referred to as <span class="math inline">\(\alpha^{*}\)</span>. That is, clusters are made as spatially contiguous as possible, with the restriction that this can never lead to an information loss in the feature space of more than 10%.</p>
<p>With <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(k^{*}\)</span> and <span class="math inline">\(\alpha^{*}\)</span> set, the final spatially constrained hierarchical clustering is performed. The output of this procedure is a single partition, in which all time series are grouped into a cluster. Some extra restrictions are imposed subsequently. Since the spatial constraint was not strict, it is not guaranteed that all the clusters in this partition are fully spatially contiguous. Clusters consisting of more than one set of spatially connected grid cells, can occur in situations where striving for full spatial contiguity would lead to a too large information loss in the feature space, and thus, clusters that would not truly represent areas with similar patterns in the distance data. In such cases, all distinct spatially contiguous areas in these clusters, will be treated as separate clusters, with their own model point. However, this may lead to a high number of model points that each represent a very small area. When this area has a high usage intensity, the described situation is acceptable. When this area has a very low usage intensity, on the other hand, it is unwanted to have a separate model point representing it. Therefore, whenever a cluster has a usage intensity of less than two bicycle pick-ups per day, it will be merged with the closest neighbouring cluster. which is found by minimizing the inter-centroid distance.</p>
<p>Now, each cluster is guaranteed to be spatially contiguous, and gets assigned one model point. Before the locations for the model points are chosen, usage data is queried from the database, and the total number of pick-ups is calculated for each grid cell. This number is assigned as a variable to the corresponding grid cell centroids. Then, for each cluster, the arithmetic means of the coordinates of all grid cell centroids in that cluster, are calculated, weighted by the number of pick-ups. Equation <a href="3-three.html#eq:eq38">(3.3)</a> shows the calculation of the weighted average latitude of a cluster, while Equation <a href="3-three.html#eq:eq39">(3.4)</a> shows the calculation of the weighted average longitude of a cluster.</p>
<p><span class="math display" id="eq:eq38">\[\begin{equation}
\phi^{*} = \frac{\sum_{i=1}^{m} \phi_{i} \times p_{i}}{\sum_{i=1}^{m} p_{i}}
\tag{3.3}
\end{equation}\]</span>
<span class="math display" id="eq:eq39">\[\begin{equation}
\lambda^{*} = \frac{\sum_{i=1}^{m} \lambda_{i} \times p_{i}}{\sum_{i=1}^{m} p_{i}}
\tag{3.4}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\phi^{*}\)</span> and <span class="math inline">\(\lambda^{*}\)</span> are respectively the weighted average latitude and the weighted average longitude of the cluster, <span class="math inline">\(\phi_i\)</span> and <span class="math inline">\(\lambda_{i}\)</span> are respectively the latitude and longitude of the <span class="math inline">\(i_{th}\)</span> grid cell centroid in the cluster, <span class="math inline">\(p_{i}\)</span> is the number of pick-ups in the <span class="math inline">\(i_{th}\)</span> grid cell in the cluster, and <span class="math inline">\(m\)</span> is the total number of grid cells in the cluster.</p>
<p>The combination {<span class="math inline">\(\phi^{*}, \lambda^{*}\)</span>} forms the coordinate pair of the weighted centroid of the cluster. This weighted centroid is then chosen to be the model point of that cluster. In this way, a model point is a cluster centroid which is dragged towards the areas where the usage intensity of the bike sharing system is higher, and where accurate forecasts are thus more important. The model points of all clusters are send to the model loop. Finally, for each cluster, grid cells are first dissolved, and then clipped by the system area, to form one geographic outline of that cluster. The geographic outlines of all clusters, stored as polygons, are send to the forecast loop. The complete methodology of the cluster loop as described above is summarized in Figure <a href="3-three.html#fig:clusterloop">3.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusterloop"></span>
<img src="figures/clusterloop.png" alt="Methodology of the cluster loop" width="\textwidth" />
<p class="caption">
Figure 3.2: Methodology of the cluster loop
</p>
</div>
</div>
<div id="threeseven" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Model loop</h2>
<p>The main purpose of the model loop is to fit time series models to the historical data of a limited set of geographical locations. These locations are called the <em>model points</em>, and result from a previous pass through the cluster loop. For each model point, <span class="math inline">\(m_{m}\)</span> weeks of distance data are queried. All data are log-transformed, to stabilize the variance, and to make sure that, when using the models for forecasting, the forecasted distances will always be larger than zero.</p>
<p>If the data are seasonal, they will pass through the decomposition process sequence. There, they will be decomposed into a trend, seasonal and remainder component with STL, as introduced in Section <a href="2-two.html#twothreethree">2.3.3</a>. Single seasonal data, with either a daily or a weekly seasonal pattern, will be decomposed once. Multiple seasonal data, that show both a daily and a weekly seasonal pattern, will first be decomposed assuming that only daily seasonality is present, after which the trend and remainder component are added together, and decomposed again, now assuming weekly seasonality. Hence, such data are eventually decomposed in a trend, remainder and two seasonal components. Since STL is performed on the log transformation of the original data, this indirectly implies that the original data are decomposed in a multiplicative way.</p>
<p>STL requires a set of parameters to be defined in advance. For most of them, there exist clear guidelines for the choice of their values, indited by <span class="citation">(<strong>cleveland1990?</strong>)</span>. Below, all STL parameters are listed, including their quantification as used in DBAFS.</p>
<ul>
<li><span class="math inline">\(n_{p}\)</span>, the number of observations per seasonal cycle. When decomposing assuming daily seasonality, <span class="math inline">\(n_{p} = 60 \times 24 / i_{s}\)</span>, and when assuming weekly seasonality, <span class="math inline">\(n_{p} = 60 \times 24 \times 7 / i_{s}\)</span>.</li>
<li><span class="math inline">\(n_{i}\)</span>, the number of passes through the inner loop within one pass through the outer loop. It should be chosen large enough such that the updating of the seasonal and trend components converges. <span class="citation">(<strong>cleveland1990?</strong>)</span> show that this convergence happens very fast, and that, inside a pass through the outer loop, only one pass through the inner loop is already sufficient. Hence, in DBAFS, <span class="math inline">\(n_{i} = 1\)</span>.</li>
<li><span class="math inline">\(n_{o}\)</span>, the number of passes through the outer loop. To have near certainty of convergence, <span class="citation">(<strong>cleveland1990?</strong>)</span> recommend ten passes through the outer loop. In R, to be extra safe, the default is set to fifteen passes. This will not be changed in DBAFS, hence <span class="math inline">\(n_{o} = 15\)</span>.</li>
<li><span class="math inline">\(n_{s}\)</span>, the seasonal smoothing parameter. It should be chosen large enough to avoid overfitting, but small enough to allow slight variations over time. The choice of <span class="math inline">\(n_{s}\)</span> is the only one where <span class="citation">(<strong>cleveland1990?</strong>)</span> propose a manual approach, that involves a visual interpretation of the time series plot. For an automated process that decomposes several time series, this is problematic. <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>, however, argue that a value of thirteen usually gives a good balance between overfitting and allowing slight variations. In DBAFS, their recommendation is used. Hence, <span class="math inline">\(n_{s} = 13\)</span>.</li>
<li><span class="math inline">\(n_{l}\)</span>, the low-pass filter smoothing parameter. <span class="citation">(<strong>cleveland1990?</strong>)</span> show that <span class="math inline">\(n_{l}\)</span> always can be set equal to the least odd integer greater than or equal to <span class="math inline">\(n_{p}\)</span>, which is done in DBAFS as well.</li>
<li><span class="math inline">\(n_{t}\)</span>, the trend smoothing parameter. It should be chosen large enough such that seasonal variation does not end up in the trend component, but small enough such that low-frequency effects do not end up in the remainder component. To achieve this goals, <span class="citation">(<strong>cleveland1990?</strong>)</span> show that <span class="math inline">\(n_{t}\)</span> should be chosen to be the smallest odd integer that satisfies the inequality <span class="math inline">\(n_{t} \geq 1.5n_{p} / (1-1.5n_{s}^{-1})\)</span>. In DBAFS, this is done as well.</li>
</ul>
<p>Once the data are decomposed in a trend component, a remainder component and one or two seasonal components, the trend and remainder are added together, and send to the ARIMA process sequence. This part of the data can be seen as the log-transformed, deseasonalized original data, and should not contain seasonal patterns anymore. Data that were originally already non-seasonal, skip the decomposition process sequence completely, and are send to the ARIMA process sequence directly after the log transformation. Both types are from now on referred to as the <em>non-seasonal data</em>.</p>
<p>In the ARIMA process sequence, an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model is fitted to the non-seasonal data, by applying the Hyndman-Khandakar algorithm, as described in Section <a href="2-two.html#twofourtwotwo">2.4.2.2</a>.2. In R, the Hyndman-Khandakar algorithm is implemented in the <code>auto.arima</code> function from the <code>forecast</code> package, with the extra restriction that the order of differencing <span class="math inline">\(d\)</span> is not allowed to be larger than two. It also allows missing values, by handling them exactly. This is an important characteristic, since it means that models will be fitted even if some observations are missing due to server errors.</p>
<p>To determine if the data of a model point should pass through the decomposition process sequence, and if yes, how many seasonal components should be subtracted, it is necessary to first identify the seasonal patterns in the data. This is done with a variation on what <span class="citation">(<strong>hyndman2018fpp?</strong>)</span> refer to as <em>time series cross-validation</em>, and works as follows. Four different seasonality options are considered:</p>
<ul>
<li>no seasonality</li>
<li>only daily seasonality</li>
<li>only weekly seasonality</li>
<li>both daily and weekly seasonality.</li>
</ul>
<p>Then, the first two of the <span class="math inline">\(n_{w}\)</span> weeks of data are selected and log-transformed. Four different models are fitted to these data, each assuming a different seasonality option. That is, when the option of no seasonality is considered, the data are directly inputted into the ARIMA process sequence. When one of the other options is considered, the data first pass through the decomposition process sequence, and then, the deseasonalized data are inputted into the ARIMA process sequence. Subsequently, each day in the first week following the ‘model building weeks,’ is forecasted separately. For the first day, this simply means that the non-seasonal data on which the ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model is build, are forecasted <span class="math inline">\(60 \times 24 / i_{s}\)</span> time lags ahead. If present, the seasonal component is forecasted in a naïve way, and added to each result. For the second day, however, one extra day of data is added, and the same ARIMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(d\)</span>,<span class="math inline">\(q\)</span>) model is used to forecast the non-seasonal part of this extended data <span class="math inline">\(60 \times 24 / i_{s}\)</span> time lags ahead, and if present, the naïve seasonal forecasts are added to the results. This process repeats for all seven days, each time using one extra day of data.</p>
<p>Once all days in this ‘forecasting week’ are forecasted, the two weeks of data on which the models were build, are now extended by one week of new data. On this three-week dataset, new models are build for each of the four seasonality options, and again, each day in the next week is forecasted separately. This shifting of the model building period keeps repeating, until there are no more weeks left to forecast. Then, for each of the seasonality options, the total RMSE of all forecasts is calculated. The option with the lowest RMSE, is the identified seasonal pattern of the data.</p>
<p>After the log-transformation, seasonality detection, possible decomposition, and model building, the following output is obtained. All model points have an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model that describes the non-seasonal part of their data. Additionally, the length of the seasonal period in their data is known. This can either be a single non-zero integer, in the case of only a daily or weekly seasonality, a vector of two non-zero integers, in the case of both a daily and a weekly seasonality, or zero, in the case of no seasonality. The chosen values <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> of the ARIMA model, the estimated parameter values <span class="math inline">\(\hat\phi_{1},...,\hat\phi_{p}\)</span> and <span class="math inline">\(\hat\theta_{1},...,\hat\theta_{q}\)</span> of the ARIMA model, and the identified length of the seasonal period, <span class="math inline">\(L_{s}\)</span>, are send to the forecast loop. The complete methodology of the model loop as described above is summarized in Figure <a href="3-three.html#fig:modelloop">3.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:modelloop"></span>
<img src="figures/modelloop.png" alt="Methodology of the model loop" width="\textwidth" />
<p class="caption">
Figure 3.3: Methodology of the model loop
</p>
</div>
</div>
<div id="threeeight" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Forecast loop</h2>
<p>The main purpose of the forecast loop is to produce a forecast whenever it is requested by a user. The forecasting task is split in two: seasonal forecasts capture the seasonal patterns in the data, while an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model takes care of the short-term dynamics. The geographical location that comes with such a forecast request, is intersected with the clusters that resulted from a previous pass through the cluster loop. This process returns the cluster in which the requested forecast location lies. Subsequently, the model structure and parameters from the model that belongs to this specific cluster, are taken from a previous pass through the model loop.</p>
<p>Since the clusters represent areas with similar patterns in the data, but not necessarily with similar means, the inherited model structure only provides the length of the seasonal patterns, and not the values corresponding to each season. Therefore, for the location of the forecast request, <span class="math inline">\(m_{f}\)</span> weeks of distance data are queried, which will be used for seasonal decomposition, whenever the inherited model structure has an identified seasonality. The length of these data should be at least twice the length of the longest seasonal pattern, plus one observation. For example, when a weekly seasonality is present in hourly data, the data needed for forecasting should at least contain <span class="math inline">\(24 \times 7 \times 2 + 1 = 337\)</span> observations.</p>
<p>The queried distance data are log-transformed. Then, they are decomposed by STL, using the identified seasonal period length(s) <span class="math inline">\(L_{s}\)</span>, which is stored in the provided model structure. When <span class="math inline">\(L_{s} = 0\)</span>, this decomposition step is skipped, and the data are treated in the same way as the combination of trend and remainder that outputs from STL. Both types are referred to as the <em>non-seasonal data</em>.</p>
<p>With the ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model that was taken from the model loop, the non-seasonal data are forecasted <span class="math inline">\(h\)</span> time lags ahead. <span class="math inline">\(h = (T_{f} - T_{c}) / i_{s}\)</span>, where <span class="math inline">\(i_{s}\)</span> is the temporal resolution of the pre-processed distance data, <span class="math inline">\(T_{c}\)</span> is the timestamp in the distance data that is closest to the time at which the forecast request was send and <span class="math inline">\(T_{f}\)</span> is the last timestamp in the interval {<span class="math inline">\(T_{c}, T_{c} + i_{s}, T_{c} + 2i_{s}, T_{c} + 3i_{s}, ..., T_{c} + ki_{s} | T_{c} + ki_{s} \leq T_{r}\)</span>}, where <span class="math inline">\(T_{r}\)</span>, in turn, is the time for which the forecast is requested. For example, imagine a situation in which the distance data contain values for each quarter of an hour, the forecast request was send at 15:48, and the forecast is requested for 16:40. Then, <span class="math inline">\(T_{c}\)</span> will be 15:45, the last timestamp in the queried distance data, and <span class="math inline">\(T_{f}\)</span> will be 16:30, the last quarterly hour timestamp before the requested forecast time. Hence, <span class="math inline">\(h = 3\)</span>. That is, a forecast meant for 16:40, will in that case effectively be a forecast for 16:30. The values of <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, and <span class="math inline">\(q\)</span> in the ARIMA model are inherited from the model structure provided by the model loop, just as the estimated parameter values <span class="math inline">\(\hat\phi_{1},...,\hat\phi_{p}\)</span> and <span class="math inline">\(\hat\theta_{1},...,\hat\theta_{q}\)</span>.</p>
<p>For the data that were decomposed, the seasonal component is forecasted separately. This is done with a seasonal naïve forecasting method, as described in Section <a href="2-two.html#twofourfour">2.4.4</a>. Then, the point forecast of the seasonal component is added to the point forecast of the non-seasonal data, to construct a reseasonalized point forecast. This point forecast is backtransformed with bias-adjustment, to the original scale of the data, as explained in Section <a href="2-two.html#twofourtwoseven">2.4.2.7</a>.7. The seasonal point forecast is also added to the upper and lower bounds of the prediction intervals, to construct reseasonalized prediction intervals, which are backtransformed with bias-adjustment as well. That is, the prediction intervals of the non-seasonal data are shifted in line with the point forecast, but do not get wider. Hence, the uncertainty of the seasonal forecasts is not included in the final prediction intervals. This keeps calculations simple, and is a reasonable approach, according to <span class="citation">(<strong>hyndman2018fpp?</strong>)</span>.</p>
<p>The forecasted distance to the nearest available bike, for the requested time and location, together with the corresponding 95% prediction interval, forms the output of DBAFS, and is send back to the user. The complete methodology of the forecast loop as described above is summarized in Figure <a href="3-three.html#fig:forecastloop">3.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:forecastloop"></span>
<img src="figures/forecastloop.png" alt="Methodology of the forecast loop" width="\textwidth" />
<p class="caption">
Figure 3.4: Methodology of the forecast loop
</p>
</div>
<!-- \clearpage -->
<!-- \shipout\null -->
<!-- \stepcounter{page} -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-two.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-four.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
