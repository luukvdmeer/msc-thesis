<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 12 2.4 Time series forecasting | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</title>
  <meta name="description" content="Chapter 12 2.4 Time series forecasting | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 12 2.4 Time series forecasting | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 2.4 Time series forecasting | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems" />
  
  
  

<meta name="author" content="Lucas van der Meer">


<meta name="date" content="2019-02-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="11-time-series-components.html">
<link rel="next" href="13-time-series-clustering.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-introduction-1.html"><a href="1-introduction-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="2-context.html"><a href="2-context.html"><i class="fa fa-check"></i><b>2</b> 1.1 Context</a></li>
<li class="chapter" data-level="3" data-path="3-objective.html"><a href="3-objective.html"><i class="fa fa-check"></i><b>3</b> 1.2 Objective</a></li>
<li class="chapter" data-level="4" data-path="4-related-work.html"><a href="4-related-work.html"><i class="fa fa-check"></i><b>4</b> 1.3 Related work</a><ul>
<li class="chapter" data-level="4.1" data-path="4-related-work.html"><a href="4-related-work.html#forecasting-in-station-based-systems"><i class="fa fa-check"></i><b>4.1</b> 1.3.1 Forecasting in station-based systems</a></li>
<li class="chapter" data-level="4.2" data-path="4-related-work.html"><a href="4-related-work.html#forecasting-in-dockless-systems"><i class="fa fa-check"></i><b>4.2</b> 1.3.2 Forecasting in dockless systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-approach.html"><a href="5-approach.html"><i class="fa fa-check"></i><b>5</b> 1.4 Approach</a></li>
<li class="chapter" data-level="6" data-path="6-outline.html"><a href="6-outline.html"><i class="fa fa-check"></i><b>6</b> 1.5 Outline</a></li>
<li class="chapter" data-level="7" data-path="7-references.html"><a href="7-references.html"><i class="fa fa-check"></i><b>7</b> References</a></li>
<li class="chapter" data-level="8" data-path="8-theoretical-background.html"><a href="8-theoretical-background.html"><i class="fa fa-check"></i><b>8</b> Theoretical background</a></li>
<li class="chapter" data-level="9" data-path="9-time-series-definition.html"><a href="9-time-series-definition.html"><i class="fa fa-check"></i><b>9</b> 2.1 Time series definition</a></li>
<li class="chapter" data-level="10" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html"><i class="fa fa-check"></i><b>10</b> 2.2 Time series characteristics</a><ul>
<li class="chapter" data-level="10.1" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#autocorrelation"><i class="fa fa-check"></i><b>10.1</b> 2.2.1 Autocorrelation</a></li>
<li class="chapter" data-level="10.2" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#stationarity"><i class="fa fa-check"></i><b>10.2</b> 2.2.2 Stationarity</a></li>
<li class="chapter" data-level="10.3" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#spectral-entropy"><i class="fa fa-check"></i><b>10.3</b> 2.2.3 Spectral entropy</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-time-series-components.html"><a href="11-time-series-components.html"><i class="fa fa-check"></i><b>11</b> 2.3 Time series components</a><ul>
<li class="chapter" data-level="11.1" data-path="11-time-series-components.html"><a href="11-time-series-components.html#definitions"><i class="fa fa-check"></i><b>11.1</b> 2.3.1 Definitions</a></li>
<li class="chapter" data-level="11.2" data-path="11-time-series-components.html"><a href="11-time-series-components.html#classical-decomposition"><i class="fa fa-check"></i><b>11.2</b> 2.3.2 Classical decomposition</a></li>
<li class="chapter" data-level="11.3" data-path="11-time-series-components.html"><a href="11-time-series-components.html#stl-decomposition"><i class="fa fa-check"></i><b>11.3</b> 2.3.3 STL decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html"><i class="fa fa-check"></i><b>12</b> 2.4 Time series forecasting</a><ul>
<li class="chapter" data-level="12.1" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>12.1</b> 2.4.1 Forecasting models</a></li>
<li class="chapter" data-level="12.2" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#arima"><i class="fa fa-check"></i><b>12.2</b> 2.4.2 ARIMA</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#structure"><i class="fa fa-check"></i><b>12.2.1</b> 2.4.2.1 Structure</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#model-selection"><i class="fa fa-check"></i><b>12.2.2</b> 2.4.2.2 Model selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#parameter-estimation"><i class="fa fa-check"></i><b>12.2.3</b> 2.4.2.3 Parameter estimation</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#model-checking"><i class="fa fa-check"></i><b>12.2.4</b> 2.4.2.4 Model checking</a></li>
<li class="chapter" data-level="12.2.5" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#forecasting"><i class="fa fa-check"></i><b>12.2.5</b> 2.4.2.5 Forecasting</a></li>
<li class="chapter" data-level="12.2.6" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#accuracy-evaluation"><i class="fa fa-check"></i><b>12.2.6</b> 2.4.2.6 Accuracy evaluation</a></li>
<li class="chapter" data-level="12.2.7" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#transformations"><i class="fa fa-check"></i><b>12.2.7</b> 2.4.2.7 Transformations</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#naive-forecasts"><i class="fa fa-check"></i><b>12.3</b> 2.4.3 Naïve forecasts</a></li>
<li class="chapter" data-level="12.4" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#seasonal-forecasts"><i class="fa fa-check"></i><b>12.4</b> 2.4.4 Seasonal forecasts</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html"><i class="fa fa-check"></i><b>13</b> 2.5 Time series clustering</a><ul>
<li class="chapter" data-level="13.1" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#dissimilarity-measures"><i class="fa fa-check"></i><b>13.1</b> 2.5.1 Dissimilarity measures</a></li>
<li class="chapter" data-level="13.2" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>13.2</b> 2.5.2 Hierarchical clustering</a></li>
<li class="chapter" data-level="13.3" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#spatial-time-series-clustering"><i class="fa fa-check"></i><b>13.3</b> 2.5.3 Spatial time series clustering</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-references-1.html"><a href="14-references-1.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="chapter" data-level="15" data-path="15-system-architecture.html"><a href="15-system-architecture.html"><i class="fa fa-check"></i><b>15</b> System architecture</a></li>
<li class="chapter" data-level="16" data-path="16-overall-design.html"><a href="16-overall-design.html"><i class="fa fa-check"></i><b>16</b> 3.1 Overall design</a></li>
<li class="chapter" data-level="17" data-path="17-software.html"><a href="17-software.html"><i class="fa fa-check"></i><b>17</b> 3.2 Software</a></li>
<li class="chapter" data-level="18" data-path="18-system-area.html"><a href="18-system-area.html"><i class="fa fa-check"></i><b>18</b> 3.3 System area</a></li>
<li class="chapter" data-level="19" data-path="19-database.html"><a href="19-database.html"><i class="fa fa-check"></i><b>19</b> 3.4 Database</a><ul>
<li class="chapter" data-level="19.1" data-path="19-database.html"><a href="19-database.html#distance-data"><i class="fa fa-check"></i><b>19.1</b> 3.4.1 Distance data</a></li>
<li class="chapter" data-level="19.2" data-path="19-database.html"><a href="19-database.html#usage-data"><i class="fa fa-check"></i><b>19.2</b> 3.4.2 Usage data</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="20-forecast-request.html"><a href="20-forecast-request.html"><i class="fa fa-check"></i><b>20</b> 3.5 Forecast request</a></li>
<li class="chapter" data-level="21" data-path="21-cluster-loop.html"><a href="21-cluster-loop.html"><i class="fa fa-check"></i><b>21</b> 3.6 Cluster loop</a></li>
<li class="chapter" data-level="22" data-path="22-model-loop.html"><a href="22-model-loop.html"><i class="fa fa-check"></i><b>22</b> 3.7 Model loop</a></li>
<li class="chapter" data-level="23" data-path="23-forecast-loop.html"><a href="23-forecast-loop.html"><i class="fa fa-check"></i><b>23</b> 3.8 Forecast loop</a></li>
<li class="chapter" data-level="24" data-path="24-references-2.html"><a href="24-references-2.html"><i class="fa fa-check"></i><b>24</b> References</a></li>
<li class="chapter" data-level="25" data-path="25-data-and-experimental-design.html"><a href="25-data-and-experimental-design.html"><i class="fa fa-check"></i><b>25</b> Data and experimental design</a></li>
<li class="chapter" data-level="26" data-path="26-data-source.html"><a href="26-data-source.html"><i class="fa fa-check"></i><b>26</b> 4.1 Data source</a></li>
<li class="chapter" data-level="27" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html"><i class="fa fa-check"></i><b>27</b> 4.2 Data retrieval</a><ul>
<li class="chapter" data-level="27.1" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html#distance-data-1"><i class="fa fa-check"></i><b>27.1</b> 4.2.1 Distance data</a></li>
<li class="chapter" data-level="27.2" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html#usage-data-1"><i class="fa fa-check"></i><b>27.2</b> 4.2.2 Usage data</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="28-experimental-design.html"><a href="28-experimental-design.html"><i class="fa fa-check"></i><b>28</b> 4.3 Experimental design</a><ul>
<li class="chapter" data-level="28.1" data-path="28-experimental-design.html"><a href="28-experimental-design.html#training-and-test-periods"><i class="fa fa-check"></i><b>28.1</b> 4.3.1 Training and test periods</a></li>
<li class="chapter" data-level="28.2" data-path="28-experimental-design.html"><a href="28-experimental-design.html#additional-software"><i class="fa fa-check"></i><b>28.2</b> 4.3.3 Additional software</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="29-references-3.html"><a href="29-references-3.html"><i class="fa fa-check"></i><b>29</b> References</a></li>
<li class="chapter" data-level="30" data-path="30-results-and-discussion.html"><a href="30-results-and-discussion.html"><i class="fa fa-check"></i><b>30</b> Results and discussion</a></li>
<li class="chapter" data-level="31" data-path="31-clustering.html"><a href="31-clustering.html"><i class="fa fa-check"></i><b>31</b> 5.1 Clustering</a></li>
<li class="chapter" data-level="32" data-path="32-model-building.html"><a href="32-model-building.html"><i class="fa fa-check"></i><b>32</b> 5.2 Model building</a></li>
<li class="chapter" data-level="33" data-path="33-forecasting-1.html"><a href="33-forecasting-1.html"><i class="fa fa-check"></i><b>33</b> 5.3 Forecasting</a></li>
<li class="chapter" data-level="34" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html"><i class="fa fa-check"></i><b>34</b> 5.4 Limitations &amp; Recommendations</a><ul>
<li class="chapter" data-level="34.1" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#limits-of-forecastability"><i class="fa fa-check"></i><b>34.1</b> 5.4.1 Limits of forecastability</a></li>
<li class="chapter" data-level="34.2" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#exogeneous-variables"><i class="fa fa-check"></i><b>34.2</b> 5.4.2 Exogeneous variables</a></li>
<li class="chapter" data-level="34.3" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#non-normal-distributions"><i class="fa fa-check"></i><b>34.3</b> 5.4.3 Non-normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="35-references-4.html"><a href="35-references-4.html"><i class="fa fa-check"></i><b>35</b> References</a></li>
<li class="chapter" data-level="36" data-path="36-conclusion.html"><a href="36-conclusion.html"><i class="fa fa-check"></i><b>36</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references-5.html"><a href="references-5.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="time-series-forecasting" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> 2.4 Time series forecasting</h1>
<div id="forecasting-models" class="section level2">
<h2><span class="header-section-number">12.1</span> 2.4.1 Forecasting models</h2>
<p>Often, the main aim of time series analysis is to forecast future values of a time series. In some cases, this can be done by using external exploratory variables. One could for example try to forecast the profit of ice cream sales by using air temperature as an exploratory variable in a linear regression model. However, there are several reasons not to forecast time series in this way, as summed up by <span class="citation">Hyndman &amp; Athanasopoulos (2018)</span>. Firstly, the underlying system of the forecasted time series may not be sufficiently understood, and even if it is, the relations with exploratory variables may be too complex. Secondly, when forecasting future values of a time series, also the future values of the exploratory variables should be known, which means that each exploratory variable should be forecasted separately before the response variable can be forecasted. This may be too difficult to do accurately, and even when it is possible, it remains a very time consuming task. Especially when the only aim is to know what will happen, and not why it will happen, it is not worth the effort. Finally, modelling a time series with conventional statistical method like linear regression will likely result in model errors that exhibit autocorrelation, which implies that such models are not able to capture all the dynamics of the data. Thus, produced forecast are not as efficient, and, probably, not as accurate as can be.</p>
<p>Instead, in time series analysis, the internal dependence structure of a time series is used to forecast future values as a function of the current and past values <span class="citation">(Shumway &amp; Stoffer, 2011)</span>. Obviously, this primarily requires a good understanding of that structure, which is obtained by describing the process that generated the data with a time series model, as defined in Definition 7, adapted from <span class="citation">Brockwell &amp; Davis (2002)</span>.</p>
<p><strong>Definition 7</strong> A <em>time series model</em> for an observed realization {<span class="math inline">\(y_{t}\)</span>} of a time series {<span class="math inline">\(Y_{t}\)</span>} is a specification of the joint distributions, or possibly only the means, variances and covariances, of the random variables that {<span class="math inline">\(Y_{t}\)</span>} comprises. <span class="math inline">\(\blacksquare\)</span></p>
<p>One of the most famous and widely used groups of time series models is known as the <em>autoregressive integrated moving average</em> (ARIMA) class of models, developed by <span class="citation">Box &amp; Jenkins (1970)</span>. In this thesis, ARIMA is used as well. The next section gives a summary of its theory, based on <span class="citation">Brockwell &amp; Davis (2002)</span>, Chapter 5, <span class="citation">Shumway &amp; Stoffer (2011)</span>, Chapter 3, and <span class="citation">Hyndman &amp; Athanasopoulos (2018)</span>, Chapter 3 and 8.</p>
</div>
<div id="arima" class="section level2">
<h2><span class="header-section-number">12.2</span> 2.4.2 ARIMA</h2>
<div id="structure" class="section level3">
<h3><span class="header-section-number">12.2.1</span> 2.4.2.1 Structure</h3>
<p>An ARIMA model is a combination of an <em>autoregressive</em> (AR) and <em>moving average</em> (MA) model, preceded by a differencing operation on the original data. An autoregressive model of order <span class="math inline">\(p\)</span>, commonly referred to as an AR(<span class="math inline">\(p\)</span>) model, is based on the assumption that the current value of a time series is a linear combination of <span class="math inline">\(p\)</span> previous values, as showed in Equation 13.</p>
<p><span class="math display">\[ y_{t} = \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + ... + \phi_{p}y_{t-p} + \epsilon_{t} \]</span></p>
<p>Where <span class="math inline">\(y_{t}\)</span> is the current value of the time series at time period <span class="math inline">\(t\)</span>, <span class="math inline">\(\epsilon_{t}\)</span> is the random error (i.e. white noise) at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\phi_{1},...,\phi_{p}\)</span> are model parameters.</p>
<p>A moving average model of order <span class="math inline">\(q\)</span>, commonly referred to as an MA(<span class="math inline">\(q\)</span>) model, is based on the assumption that the current value of a time series is a linear combination of <span class="math inline">\(q\)</span> previous errors, as showed in Equation 14.</p>
<p><span class="math display">\[ y_{t} = \epsilon_{t} + \theta_{1}\epsilon_{t-1} + \theta_{2}\epsilon_{t-2} + ... + \theta_{q}\epsilon_{t-q} \]</span></p>
<p>Where <span class="math inline">\(y_{t}\)</span> is the current value of the time series at time period <span class="math inline">\(t\)</span>, <span class="math inline">\(\epsilon_{t}\)</span> is the error at time period <span class="math inline">\(t\)</span>, which is assumed to be white noise, and <span class="math inline">\(\theta_{1},...,\theta_{q}\)</span> are model parameters.</p>
<p>AR(<span class="math inline">\(p\)</span>) and MA(<span class="math inline">\(q\)</span>) models can be combined into an autoregressive moving average model of order (<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>), commonly referred to as ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>). That is, in such a model, the current value of a time series is a linear combination of both <span class="math inline">\(p\)</span> previous values and <span class="math inline">\(q\)</span> previous errors, as showed in Equation 15.</p>
<p><span class="math display">\[ 
y_{t} = \phi_{1}y_{t-1} + ... + \phi_{p}y_{t-p} + \theta_{1}\epsilon_{t-1} + ... + \theta_{q}\epsilon_{t-q} + \epsilon_{t} 
\]</span></p>
<p>ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>) models require the forecasted time series to be stationary. When working with non-stationary time series, it is often possible to stationarize the series by differencing it one or more times. The first order difference of a time series is the series of changes from one time period to the next, as shown in Equation 16.</p>
<p><span class="math display">\[ \nabla y_{t} = y_{t} - y_{t-1} \]</span></p>
<p>Where <span class="math inline">\(\nabla y_{t}\)</span> is the first order difference of <span class="math inline">\(y_{t}\)</span>. When the first order difference is still non-stationary, the second order difference <span class="math inline">\(\nabla^{2}y_{t}\)</span> can be computed by taking again the first order difference of <span class="math inline">\(\nabla y_{t}\)</span>, and so on. The original non-stationary time series that needed to be differenced in order to get stationary, is called an <em>integrated</em> version of the stationary series. That is why a model that first stationarizes the data by applying a <span class="math inline">\(d\)</span>-th order difference, before fitting an ARMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>) model, is called an autoregressive integrated moving average model of order (<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>), commonly referred to as ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>). That is, in such a model, the current value of the <span class="math inline">\(d-\)</span>th order difference of a time series is a linear combination of both <span class="math inline">\(p\)</span> previous values and <span class="math inline">\(q\)</span> previous errors, as showed in Equation 17.</p>
<p><span class="math display">\[ 
\nabla^{d}y_{t} = \phi_{1}\nabla^{d}y_{t-1} + ... + \phi_{p}\nabla^{d}y_{t-p} + \theta_{1}\epsilon_{t-1} + ... + \theta_{q}\epsilon_{t-q} + \epsilon_{t} 
\]</span></p>
<p>Where <span class="math inline">\(\nabla^{d}y_{t}\)</span> is the <span class="math inline">\(d\)</span>-th order difference of <span class="math inline">\(y_{t}\)</span>. Note here that ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) is a general form of all the other models discussed earlier in this section. For example, an AR(1) model can also be written as ARIMA(1,0,0), an ARMA(2,1) as ARIMA(1,0,2), and so on.</p>
<p>The process of finding an appropriate ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model that represents a time series is known as the Box-Jenkins modelling procedure and consists of three stages, named model selection, parameter estimation and model checking. All these stages are described separately in the next three subsections.</p>
</div>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">12.2.2</span> 2.4.2.2 Model selection</h3>
<p>In the model selection stage, <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are chosen. In this process, <span class="math inline">\(d\)</span> is selected first, such that the choice of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> will be based on a stationary time series. An appropriate value for <span class="math inline">\(d\)</span> can be found by inspecting the plotted data <span class="math inline">\(y_{t}\)</span>, with time on the x-axis, and define visually if the data are stationary. If not, then difference the data once, and inspect the plot of <span class="math inline">\(\nabla y_{t}\)</span>. If <span class="math inline">\(\nabla y_{t}\)</span> does not seem stationary either, take the second-order difference <span class="math inline">\(\nabla^{2} y_{t}\)</span>, and so on. In general, however, it is not recommended to difference more than two times. As an addition to the time plots, plotting the sample autocorrelation function of the data can help to identify stationarity. Non-stationary data show a slow decay in autocorrelation as the time lag increases, while for stationary data, the autocorrelation will drop to zero relatively fast.</p>
<p>Once <span class="math inline">\(d\)</span> has been set, either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> can be selected by inspecting the autocorrelation function plot and the partial autocorrelation function plot of the differenced data, which respectively plot the sample autocorrelation function (ACF), defined in Equation 2.x, and the sample partial autocorrelation function (PACF), for several lags <span class="math inline">\(h\)</span>. The PACF is the relationship between an observation at time <span class="math inline">\(t\)</span> and and observation at time <span class="math inline">\(t-k\)</span>, removing the effects of all time lags in between, i.e. <span class="math inline">\(1, 2, ..., k-1\)</span>. Then, appropriate values for either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> are found with the following rules of thumb:</p>
<ul>
<li>The PACF plot of an ARIMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(d\)</span>,<span class="math inline">\(0\)</span>) process cuts of after lag <span class="math inline">\(p\)</span>, and the ACF plot tails off.</li>
<li>The ACF plot of an ARIMA(<span class="math inline">\(0\)</span>,<span class="math inline">\(d\)</span>,<span class="math inline">\(q\)</span>) process cuts of after lag <span class="math inline">\(q\)</span>, and the PACF plot tails off.</li>
</ul>
<p>When dealing with ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) processes where both <span class="math inline">\(p &gt; 0\)</span> and <span class="math inline">\(q &gt; 0\)</span>, the ACF plot and PACF plot will both tail off, and finding appropriate values for <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> turns into a trial-and-error approach, where models with different combinations of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are compared.</p>
<p>The methodology as described above is used often, but involves a lot of manual interventions. This makes it a rather subjective way of working, that is labour intensive, especially when a large number of time series needs to be modelled, and requires expert knowledge. Therefore, several automated approaches to select <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> have been proposed. One of them is the Hyndman-Khandakar algorithm, which methodology is summarized below, in a simplified way. For the full details, see <span class="citation">Hyndman &amp; Khandakar (2008)</span>.</p>
<p><strong>Step 1.</strong> To define <span class="math inline">\(d\)</span>, the Hyndman-Khandakar algorithm uses the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which is a statistical test used to determine stationarity of a time series. Only if there is enough statistical evidence, the null hypothesis that the time series is stationary will be rejected, and the time series is instead considered to be non-stationary. The detailed mathematics underlying the test can be found in <span class="citation">Kwiatkowski, Phillips, Schmidt, &amp; Shin (1992)</span>.</p>
<p>Using the KPSS test, first, the original data <span class="math inline">\(y_{t}\)</span> are tested for stationarity. When <span class="math inline">\(y_{t}\)</span> are considered stationary, <span class="math inline">\(d = 0\)</span>, and when considered non-stationary, the first-order differenced data <span class="math inline">\(\nabla y_{t}\)</span> are tested for stationarity. Again, when <span class="math inline">\(\nabla y_{t}\)</span> are considered stationary, <span class="math inline">\(d = 1\)</span>, and when considered non-stationary, the second-order differenced data <span class="math inline">\(\nabla^{2} y_{t}\)</span> are tested for stationarity. This process is repeated until a stationary series is obtained.</p>
<p><strong>Step 2.</strong> In the second step, four different models are fitted to the <span class="math inline">\(d\)</span>-times differenced data.</p>
<ul>
<li>An ARIMA(<span class="math inline">\(0\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(0\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(1\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(0\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(0\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(1\)</span>) model.</li>
<li>An ARIMA(<span class="math inline">\(2\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(2\)</span>) model.</li>
</ul>
<p>Then, the model with the lowest AIC is selected. AIC, which stands for Aikake’s Information Criterion, is a measure for the goodness-of-fit of a model, and can be calculated with Equation 2.x.</p>
<p><span class="math display">\[ AIC = -2 \log(L) + 2k\]</span></p>
<p>Where <span class="math inline">\(L\)</span> is the likelihood of the data, and <span class="math inline">\(k\)</span> is the number of free parameters in the model. In this case, <span class="math inline">\(k = p + q + l + 1\)</span>, where <span class="math inline">\(l = 1\)</span> when a non-zero constant is included, and <span class="math inline">\(l = 0\)</span> otherwise. The ‘<span class="math inline">\(+1\)</span>’ term is included, since the variance of the residuals is also a parameter. To find the best fitting model, AIC should be minimized. The idea behind AIC is the following. The likelihood monotonically increases when more parameters are added to the model, and therefore, only maximizing the likelihood would favor a model that overfits the data. AIC prevents such overfitting, by penalizing the likelihood with a term that is proportional to the number of parameters used in the model.</p>
<p><strong>Step 3.</strong> In the third step, several variations of the model that was selected in step 2, are fitted to the <span class="math inline">\(d\)</span>-times differenced data. These variations include:</p>
<ul>
<li>Models where either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> vary <span class="math inline">\(\pm 1\)</span> from the selected model, given that <span class="math inline">\(p, q \ngtr 5\)</span>.</li>
<li>Models where both <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> vary <span class="math inline">\(\pm 1\)</span> from the selected model, given that <span class="math inline">\(p, q \ngtr 5\)</span>.</li>
</ul>
<p>From the selected model and all its variations, the model with the lowest AIC is chosen to be the new selected model, and step 3 is repeated. The algorithm stops when there are no variations of the selected model that have a lower AIC. In that case, the selected model is the optimal model, and forms the output of the Hyndman-Khandakar algorithm. The complete methodology of the algorithm as described above is summarized in Figure 2.x.</p>
<div class="figure" style="text-align: center"><span id="fig:hyndman"></span>
<img src="Figures/hyndman.png" alt="Summary of the Hyndman-Khandakar algorithm" width="\textwidth" />
<p class="caption">
Figure 12.1: Summary of the Hyndman-Khandakar algorithm
</p>
</div>
</div>
<div id="parameter-estimation" class="section level3">
<h3><span class="header-section-number">12.2.3</span> 2.4.2.3 Parameter estimation</h3>
<p>When <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are defined, the model parameters <span class="math inline">\(\phi_{1},...,\phi_{p}\)</span> and <span class="math inline">\(\theta_{1},...,\theta_{q}\)</span> need to be estimated. Usually, this is done with <em>maximum likelihood estimation</em> (MLE). The likelihood is the probability of obtaining the observed data, given the model and specific parameter values. The parameter values that maximize the likelihood are called the maximum likelihood estimators of the true parameters, and will be used as the parameter estimates in the fitted ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model, which then is referred to as the maximum likelihood ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model. The detailed mathematical description of MLE for ARIMA models can be found in <span class="citation">Brockwell &amp; Davis (2002)</span>, section 5.2.</p>
<p>Note here that the Hyndman-Khandakar algorithm already produces a fitted model as output, and the parameter estimation as described in this section is done inside the algorithm, each time a model is fitted to the <span class="math inline">\(d\)</span>-times differenced data.</p>
</div>
<div id="model-checking" class="section level3">
<h3><span class="header-section-number">12.2.4</span> 2.4.2.4 Model checking</h3>
<p>Model checking involves identifying if the fitted model is adequate. This is done by inspecting its residuals, which are defined as the difference between the actual observations and the corresponding fitted values, as shown in Equation 2.x.</p>
<p><span class="math display">\[ \epsilon_{t} = y_{t} - \hat{y}_{t} \]</span></p>
<p>If the maximum likelihood ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model is the true process that generated the data, the residuals should be completely white noise. Recall, however, that the model is an estimation of the true process. Therefore, a good model that fits the data well, should have residuals with properties that <em>approximately</em> reflect those of white noise, i.e. a zero mean and no autocorrelation. If autocorrelation is present in the residuals, this means that there is still information left in the data, which could be used to create more accurate forecasts. A non-zero mean will lead to biased forecasts.</p>
<p>Autocorrelation in the residuals can be detected by a visual interpretation of the residual ACF plot, which will always show some autocorrelation, due to random variation. Therefore, given that <span class="math inline">\(n\)</span> is the length of the modelled time series, and assuming a normal distribution, the residuals are considered to be uncorrelated when for at least 95% of the time lags, the residual ACF lies within the interval <span class="math inline">\([-1.96/\sqrt{n}, 1.96/\sqrt{n}]\)</span>.</p>
<p>Usually, several computations within the model fitting and forecasting process build on the assumption that the data come from a normally distributed population. For example, in MLE and the calculation of AIC, Gaussian likelihood is commonly used. Furthermore, prediction intervals of forecasts are in general derived from the normal distribution. Normally distributed residuals indicate that these assumptions were valid, and are therefore a valuable property of a model. However, as stated by <span class="citation">Brockwell &amp; Davis (2002)</span>, using Gaussian likelihood is sensible even when the data are not normally distributed.</p>
</div>
<div id="forecasting" class="section level3">
<h3><span class="header-section-number">12.2.5</span> 2.4.2.5 Forecasting</h3>
<p>A fitted ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model can then be used to forecast the future values of a time series. To do so, Equation 2.x is rewritten, such that the current value of the time series, <span class="math inline">\(y_{t}\)</span>, is replaced by a future value of the time series, <span class="math inline">\(y_{t+h}\)</span>, as showed in Equation 2.x.</p>
<p><span class="math display">\[ 
\nabla^{d}y_{t+h} = \hat\phi_{1}\nabla^{d}y_{t+h-1} + ... + \hat\phi_{p}\nabla^{d}y_{t+h-p} +\hat\theta_{1}\epsilon_{t+h-1} + ... + \hat\theta_{q}\epsilon_{t+h-q} + \epsilon_{t+1} 
\]</span></p>
<p>Where <span class="math inline">\(h\)</span> is the forecast horizon, i.e. the number of time lags ahead at which the forecast is made, <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span> and <span class="math inline">\(q\)</span> are known and constant, and <span class="math inline">\(\hat\phi_{1},...,\hat\phi_{p}\)</span> and <span class="math inline">\(\hat\theta_{1},...,\hat\theta_{q}\)</span> are the estimated parameter values, which are also constant.</p>
<p>When <span class="math inline">\(h &gt; 1\)</span>, more than one forecast has to be made. For example, the forecast of <span class="math inline">\(\nabla^{d}y_{t+2}\)</span>, the value of the time series two time lags ahead, is based on <span class="math inline">\(\nabla^{d}y_{t+2-1}\)</span>, the value of the time series one time lag ahead. Therefore, <span class="math inline">\(\nabla^{d}y_{t+2-1}\)</span> needs to be forecasted first, before <span class="math inline">\(\nabla^{d}y_{t+2}\)</span> can be forecasted. In general, this means that the uncertainty of the forecasts increases as <span class="math inline">\(h\)</span> increases. This uncertainty is expressed by means of a prediction interval. Most often, the 95% prediction interval is used. Assuming normally distributed residuals, the lower and upper bound of the 95% prediction interval for the <span class="math inline">\(h\)</span>-step forecast can be calculated with Equation 2.x and 2.x, respectively.</p>
<p><span class="math display">\[ \ell = \hat{y}_{t+h} - 1.96\hat\sigma_{h}\]</span> <span class="math display">\[ \upsilon = \hat{y}_{t+h} + 1.96\hat\sigma_{h}\]</span></p>
<p>Where <span class="math inline">\(\ell\)</span> is the lower bound of the 95% prediction interval, <span class="math inline">\(\upsilon\)</span> is the upper bound of the 95% prediction interval, <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted value <span class="math inline">\(h\)</span> time lags ahead. <span class="math inline">\(\hat\sigma_{h}\)</span> is the estimated standard deviation of the forecast distribution <span class="math inline">\(h\)</span> time lags ahead, which is explained below. The 95% prediction interval can be interpreted as follows: there is a 95% probability that <span class="math inline">\(\ell \leq {y}_{t+h} \leq \upsilon\)</span>.</p>
<p>Recall that in Definition 1, a time series was defined as a collection of random variables. In fact, to state it statistically correct, it is the distribution of the random variable <span class="math inline">\(h\)</span> time lags ahead that is forecasted, rather than an individual value. This distribution is referred to as the forecast distribution, and the single forecasted value, also known as the <em>point forecast</em>, is then taken to be the mean of the forecast distribution. In Equation 2.x and 2.x, <span class="math inline">\(\hat\sigma_{h}\)</span> is the estimated standard deviation of the forecasted distribution, assuming it is a normal distribution with mean <span class="math inline">\({y}_{t+h}\)</span> and variance <span class="math inline">\(\sigma_{h}^{2}\)</span>. When <span class="math inline">\(h = 1\)</span>, the residual standard deviation <span class="math inline">\(\sigma_{\epsilon}\)</span> is a good estimate for <span class="math inline">\(\sigma_{h}\)</span>. However, for <span class="math inline">\(h &gt; 1\)</span>, computations get more complex. For a detailed description, see <span class="citation">Shumway &amp; Stoffer (2011)</span>, section 3.5.</p>
</div>
<div id="accuracy-evaluation" class="section level3">
<h3><span class="header-section-number">12.2.6</span> 2.4.2.6 Accuracy evaluation</h3>
<p>A good model fit, does not necessarily lead to accurate forecasts. Therefore, when evaluating its performance, the forecasting model should be used to forecast multiple values of new data that were not included in the model building process. The error of each individual forecast can be calculated with Equation 2.x.</p>
<p><span class="math display">\[ e_{t+h} = y_{t+h} - \hat{y}_{t+h} \]</span></p>
<p>Where <span class="math inline">\(y_{t+h}\)</span> is the observed data value <span class="math inline">\(h\)</span> time lags into the future, and <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted data value <span class="math inline">\(h\)</span> time lags into the future. Obviously, future in this sense is relative to the model building period.</p>
<p>When making <span class="math inline">\(k\)</span> different forecasts, the corresponding forecast errors <span class="math inline">\(e_{1}, e_{2}, ..., e_{k}\)</span>, can be summarized with an error metric. Several of those metrics exist. Some of them are only applicable to errors that all have the same units, while others may also be used when errors with different units are compared. Since all forecasts in this thesis are distances, the unit-dependent errors are adequate. Most commonly used are the Mean Absolute Error (MAE), which can be calculated with Equation 2.x, and the Root Mean Squared Error (RMSE), which can be calculated with Equation 2.x.</p>
<p><span class="math display">\[ MAE = \frac{\sum_{i = 1}^k |e_{i}|}{k} \]</span> <span class="math display">\[ RMSE = \sqrt{\frac{\sum_{i = 1}^k e_{i}^{2}}{k}} \]</span> Both the MAE and RMSE return values that are in the same scale as the original data. The MAE gives the same weight to all errors. The RMSE, however, gives large errors more weight than small errors, and therefore penalizes a large error variance. According to <span class="citation">Chai &amp; Draxler (2014)</span>, the RMSE usually is better at revealing differences in model performance.</p>
<p>In some of the works discussed in section 1.3, such as <span class="citation">Y. Li et al. (2015)</span>, <span class="citation">Z. Yang et al. (2016)</span> and <span class="citation">Lozano et al. (2018)</span>, the Root Mean Squared Logarithmic Error (RMSLE) is reported instead of the RMSE. Here, the natural logarithms of the observed and forecasted data values are used in the forecast error computation. The main reason for doing so, is that larger errors, usually occuring during peak hours or in areas/stations with a high usage intensity, do not dominate smaller errors.</p>
</div>
<div id="transformations" class="section level3">
<h3><span class="header-section-number">12.2.7</span> 2.4.2.7 Transformations</h3>
<p>Often, forecasts can be improved by using mathematical transformations, such that the original data is adjusted for some known patterns causing non-stationary and/or non-linear behaviour. That is, the data are transformed in advance, and the modelling and forecasting procedures are applied to the transformed data. After forecasting the transformed data, forecasted values on the original scale are obtained based upon the inverse transformation, a process that is commonly called <em>back transforming</em>. A particularly useful transformation is the <em>log transformation</em>, which suppresses larger fluctuations that occur when the level of the time series increases. Furthermore, they guarantee strictly positive forecasted values. The log transformed data <span class="math inline">\(\omega_{t}\)</span>, is derived by taking the natural logarithm of the original data <span class="math inline">\(y_{t}\)</span>, as showed in Equation 2.x.</p>
<p><span class="math display">\[ \omega_{t} = \log y_{t} \]</span></p>
<p>However, care has to be taken when back transforming a log transformed forecast to the original scale. Intuitively, one would obtain the back transformed forecast <span class="math inline">\(\hat{y}_{t+h}\)</span> by setting it equal to <span class="math inline">\(e^{\hat{\omega}_{t+h}}\)</span>, where <span class="math inline">\(e\)</span> is Euler’s number. However, assuming that the forecast distribution of <span class="math inline">\(\omega_{t+h}\)</span>, <span class="math inline">\(\Omega_{t+h}\)</span>, is normal, with mean <span class="math inline">\(\mu_{\Omega_{t+h}}\)</span> and variance <span class="math inline">\(\sigma_{\Omega_{t+h}}^{2}\)</span>, then the forecast distribution of <span class="math inline">\(y_{t+h}\)</span>, <span class="math inline">\(Y_{t+h}\)</span>, follows a log-normal distribution, with mean <span class="math inline">\(\mu_{Y_{t+h}}\)</span> as defined in Equation 2.x.</p>
<p><span class="math display">\[ \mu_{Y_{t+h}} = e^{(\mu_{\Omega_{t+h}} + 0.5 \sigma_{\Omega_{t+h}}^{2})} \]</span></p>
<p>For the proof of this theorem, see <span class="citation">Dambolena, Eriksen, &amp; Kopcso (2009)</span>. <span class="citation">Hyndman &amp; Athanasopoulos (2018)</span> refer to <span class="math inline">\(\mu_{Y_{t+h}}\)</span> as the <em>bias-adjusted point forecast</em>.</p>
</div>
</div>
<div id="naive-forecasts" class="section level2">
<h2><span class="header-section-number">12.3</span> 2.4.3 Naïve forecasts</h2>
<p>It is common practice to compare the errors of forecasts obtained with a fitted model to those of forecasts obtained with a very simple forecasting method. Such a simple method, is in that case refered to as a <em>baseline method</em>. If the more sophisticated model does not lead to considerably better forecast accuracies than the baseline, it can be dismissed <span class="citation">(Hyndman &amp; Athanasopoulos, 2018)</span>.</p>
<p>One of the simplest forecast methods around, often used as a baseline, is known as the naïve method. When forecasting with the naïve method, all forecasted values will be equal to the last observation, no matter how far the forecasting window <span class="math inline">\(h\)</span> reaches, as shown in Equation 2.x.</p>
<p><span class="math display">\[ \hat{y}_{t+h} = y_{t} \]</span></p>
<p>Where <span class="math inline">\(\hat{y}_{t+h}\)</span> is the forecasted data value <span class="math inline">\(h\)</span> time lags into the future, and <span class="math inline">\(y_{t}\)</span> is the last observed data value.</p>
</div>
<div id="seasonal-forecasts" class="section level2">
<h2><span class="header-section-number">12.4</span> 2.4.4 Seasonal forecasts</h2>
<p>ARIMA models as described in section 2.4.2 are designed for data without a seasonal component. With some modifications, they can be applied to seasonal data as well. That works as follows. Instead of an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) model, an ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>)(<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>) model is fitted to the data. The (<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>) part of the model works in a similar fashion as the (<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>) part, but relates to the seasonal component of the data. Hence, <span class="math inline">\(P\)</span> is the number of seasonal autoregressive terms in the model, <span class="math inline">\(D\)</span> is the order of seasonal differencing, and <span class="math inline">\(Q\)</span> is the number of seasonal moving average terms. Where <span class="math inline">\(p = 1\)</span> means that the past observation <span class="math inline">\(y_{t-1}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>, setting <span class="math inline">\(P = 1\)</span> means that the past observation <span class="math inline">\(y_{t-m}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>, with <span class="math inline">\(m\)</span> being the number of observations per seasonal cycle. The same holds for <span class="math inline">\(Q\)</span>: setting <span class="math inline">\(Q = 1\)</span>, means that the past error <span class="math inline">\(\epsilon_{t-m}\)</span> is used to model the current value <span class="math inline">\(y_{t}\)</span>. Regarding the seasonal differencing parameter <span class="math inline">\(D\)</span>, the first order seasonal difference of a time series is the series of changes from one seasonal period to the next <span class="citation">(Shumway &amp; Stoffer, 2011)</span>.</p>
<p>However, ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>)(<span class="math inline">\(P\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(Q\)</span>), as well as several other commonly used seasonal models such as Holt Winters exponential smoothing, have two main limitations which make them unsuitable for some kind of data. Firstly, they were primarily designed to work with shorter seasonal periods, such as monthly data with patterns that repeat every year (i.e. <span class="math inline">\(m = 12\)</span>). In the case of longer seasonal periods, which may occur for example in daily and sub-daily data, modelling becomes inefficient. In the R statistical software, for example, the <code>forecast</code> package <span class="citation">(Hyndman &amp; Khandakar, 2008)</span> only allows seasonal periods up to <span class="math inline">\(m = 350\)</span> <span class="citation">(Hyndman, 2010)</span>.</p>
<p>Secondly, these models can not handle more than one seasonal pattern at a time. Again, this can be problematic especially for daily data, in which both a weekly and yearly pattern may exist, and sub-daily data, in which even a daily, weekly and yearly pattern may exist. One of the alternative approaches proposed by <span class="citation">Hyndman &amp; Athanasopoulos (2018)</span> works as follows. First, the data is decomposed into a trend, seasonal and remainder component. Then, the trend and remainder component are together modelled by a non-seasonal model, such as ARIMA(<span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(q\)</span>), and forecasted accordingly. The seasonal component, in turn, can be forecasted with a seasonal naïve method, meaning that the forecasted value will be equal to the last observed value from the same season of the year. That is, the seasonal forecast for timestamp <span class="math inline">\(y_{t+h}\)</span> will be equal to the last observed value in the sequence {<span class="math inline">\(y_{t+h-m \times 1}, y_{t+h-m \times 2}, ...\)</span>}. Then, the non-seasonal and seasonal forecasts are added back together, to obtain a single forecasted value.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11-time-series-components.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="13-time-series-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
