<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 13 2.5 Time series clustering | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</title>
  <meta name="description" content="Chapter 13 2.5 Time series clustering | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 13 2.5 Time series clustering | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 2.5 Time series clustering | Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems" />
  
  
  

<meta name="author" content="Lucas van der Meer">


<meta name="date" content="2019-02-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="12-time-series-forecasting.html">
<link rel="next" href="14-references-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-introduction-1.html"><a href="1-introduction-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="2-context.html"><a href="2-context.html"><i class="fa fa-check"></i><b>2</b> 1.1 Context</a></li>
<li class="chapter" data-level="3" data-path="3-objective.html"><a href="3-objective.html"><i class="fa fa-check"></i><b>3</b> 1.2 Objective</a></li>
<li class="chapter" data-level="4" data-path="4-related-work.html"><a href="4-related-work.html"><i class="fa fa-check"></i><b>4</b> 1.3 Related work</a><ul>
<li class="chapter" data-level="4.1" data-path="4-related-work.html"><a href="4-related-work.html#forecasting-in-station-based-systems"><i class="fa fa-check"></i><b>4.1</b> 1.3.1 Forecasting in station-based systems</a></li>
<li class="chapter" data-level="4.2" data-path="4-related-work.html"><a href="4-related-work.html#forecasting-in-dockless-systems"><i class="fa fa-check"></i><b>4.2</b> 1.3.2 Forecasting in dockless systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-approach.html"><a href="5-approach.html"><i class="fa fa-check"></i><b>5</b> 1.4 Approach</a></li>
<li class="chapter" data-level="6" data-path="6-outline.html"><a href="6-outline.html"><i class="fa fa-check"></i><b>6</b> 1.5 Outline</a></li>
<li class="chapter" data-level="7" data-path="7-references.html"><a href="7-references.html"><i class="fa fa-check"></i><b>7</b> References</a></li>
<li class="chapter" data-level="8" data-path="8-theoretical-background.html"><a href="8-theoretical-background.html"><i class="fa fa-check"></i><b>8</b> Theoretical background</a></li>
<li class="chapter" data-level="9" data-path="9-time-series-definition.html"><a href="9-time-series-definition.html"><i class="fa fa-check"></i><b>9</b> 2.1 Time series definition</a></li>
<li class="chapter" data-level="10" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html"><i class="fa fa-check"></i><b>10</b> 2.2 Time series characteristics</a><ul>
<li class="chapter" data-level="10.1" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#autocorrelation"><i class="fa fa-check"></i><b>10.1</b> 2.2.1 Autocorrelation</a></li>
<li class="chapter" data-level="10.2" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#stationarity"><i class="fa fa-check"></i><b>10.2</b> 2.2.2 Stationarity</a></li>
<li class="chapter" data-level="10.3" data-path="10-time-series-characteristics.html"><a href="10-time-series-characteristics.html#spectral-entropy"><i class="fa fa-check"></i><b>10.3</b> 2.2.3 Spectral entropy</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-time-series-components.html"><a href="11-time-series-components.html"><i class="fa fa-check"></i><b>11</b> 2.3 Time series components</a><ul>
<li class="chapter" data-level="11.1" data-path="11-time-series-components.html"><a href="11-time-series-components.html#definitions"><i class="fa fa-check"></i><b>11.1</b> 2.3.1 Definitions</a></li>
<li class="chapter" data-level="11.2" data-path="11-time-series-components.html"><a href="11-time-series-components.html#classical-decomposition"><i class="fa fa-check"></i><b>11.2</b> 2.3.2 Classical decomposition</a></li>
<li class="chapter" data-level="11.3" data-path="11-time-series-components.html"><a href="11-time-series-components.html#stl-decomposition"><i class="fa fa-check"></i><b>11.3</b> 2.3.3 STL decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html"><i class="fa fa-check"></i><b>12</b> 2.4 Time series forecasting</a><ul>
<li class="chapter" data-level="12.1" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>12.1</b> 2.4.1 Forecasting models</a></li>
<li class="chapter" data-level="12.2" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#arima"><i class="fa fa-check"></i><b>12.2</b> 2.4.2 ARIMA</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#structure"><i class="fa fa-check"></i><b>12.2.1</b> 2.4.2.1 Structure</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#model-selection"><i class="fa fa-check"></i><b>12.2.2</b> 2.4.2.2 Model selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#parameter-estimation"><i class="fa fa-check"></i><b>12.2.3</b> 2.4.2.3 Parameter estimation</a></li>
<li class="chapter" data-level="12.2.4" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#model-checking"><i class="fa fa-check"></i><b>12.2.4</b> 2.4.2.4 Model checking</a></li>
<li class="chapter" data-level="12.2.5" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#forecasting"><i class="fa fa-check"></i><b>12.2.5</b> 2.4.2.5 Forecasting</a></li>
<li class="chapter" data-level="12.2.6" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#accuracy-evaluation"><i class="fa fa-check"></i><b>12.2.6</b> 2.4.2.6 Accuracy evaluation</a></li>
<li class="chapter" data-level="12.2.7" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#transformations"><i class="fa fa-check"></i><b>12.2.7</b> 2.4.2.7 Transformations</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#naive-forecasts"><i class="fa fa-check"></i><b>12.3</b> 2.4.3 Naïve forecasts</a></li>
<li class="chapter" data-level="12.4" data-path="12-time-series-forecasting.html"><a href="12-time-series-forecasting.html#seasonal-forecasts"><i class="fa fa-check"></i><b>12.4</b> 2.4.4 Seasonal forecasts</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html"><i class="fa fa-check"></i><b>13</b> 2.5 Time series clustering</a><ul>
<li class="chapter" data-level="13.1" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#dissimilarity-measures"><i class="fa fa-check"></i><b>13.1</b> 2.5.1 Dissimilarity measures</a></li>
<li class="chapter" data-level="13.2" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>13.2</b> 2.5.2 Hierarchical clustering</a></li>
<li class="chapter" data-level="13.3" data-path="13-time-series-clustering.html"><a href="13-time-series-clustering.html#spatial-time-series-clustering"><i class="fa fa-check"></i><b>13.3</b> 2.5.3 Spatial time series clustering</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-references-1.html"><a href="14-references-1.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="chapter" data-level="15" data-path="15-system-architecture.html"><a href="15-system-architecture.html"><i class="fa fa-check"></i><b>15</b> System architecture</a></li>
<li class="chapter" data-level="16" data-path="16-overall-design.html"><a href="16-overall-design.html"><i class="fa fa-check"></i><b>16</b> 3.1 Overall design</a></li>
<li class="chapter" data-level="17" data-path="17-software.html"><a href="17-software.html"><i class="fa fa-check"></i><b>17</b> 3.2 Software</a></li>
<li class="chapter" data-level="18" data-path="18-system-area.html"><a href="18-system-area.html"><i class="fa fa-check"></i><b>18</b> 3.3 System area</a></li>
<li class="chapter" data-level="19" data-path="19-database.html"><a href="19-database.html"><i class="fa fa-check"></i><b>19</b> 3.4 Database</a><ul>
<li class="chapter" data-level="19.1" data-path="19-database.html"><a href="19-database.html#distance-data"><i class="fa fa-check"></i><b>19.1</b> 3.4.1 Distance data</a></li>
<li class="chapter" data-level="19.2" data-path="19-database.html"><a href="19-database.html#usage-data"><i class="fa fa-check"></i><b>19.2</b> 3.4.2 Usage data</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="20-forecast-request.html"><a href="20-forecast-request.html"><i class="fa fa-check"></i><b>20</b> 3.5 Forecast request</a></li>
<li class="chapter" data-level="21" data-path="21-cluster-loop.html"><a href="21-cluster-loop.html"><i class="fa fa-check"></i><b>21</b> 3.6 Cluster loop</a></li>
<li class="chapter" data-level="22" data-path="22-model-loop.html"><a href="22-model-loop.html"><i class="fa fa-check"></i><b>22</b> 3.7 Model loop</a></li>
<li class="chapter" data-level="23" data-path="23-forecast-loop.html"><a href="23-forecast-loop.html"><i class="fa fa-check"></i><b>23</b> 3.8 Forecast loop</a></li>
<li class="chapter" data-level="24" data-path="24-references-2.html"><a href="24-references-2.html"><i class="fa fa-check"></i><b>24</b> References</a></li>
<li class="chapter" data-level="25" data-path="25-data-and-experimental-design.html"><a href="25-data-and-experimental-design.html"><i class="fa fa-check"></i><b>25</b> Data and experimental design</a></li>
<li class="chapter" data-level="26" data-path="26-data-source.html"><a href="26-data-source.html"><i class="fa fa-check"></i><b>26</b> 4.1 Data source</a></li>
<li class="chapter" data-level="27" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html"><i class="fa fa-check"></i><b>27</b> 4.2 Data retrieval</a><ul>
<li class="chapter" data-level="27.1" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html#distance-data-1"><i class="fa fa-check"></i><b>27.1</b> 4.2.1 Distance data</a></li>
<li class="chapter" data-level="27.2" data-path="27-data-retrieval.html"><a href="27-data-retrieval.html#usage-data-1"><i class="fa fa-check"></i><b>27.2</b> 4.2.2 Usage data</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="28-experimental-design.html"><a href="28-experimental-design.html"><i class="fa fa-check"></i><b>28</b> 4.3 Experimental design</a><ul>
<li class="chapter" data-level="28.1" data-path="28-experimental-design.html"><a href="28-experimental-design.html#training-and-test-periods"><i class="fa fa-check"></i><b>28.1</b> 4.3.1 Training and test periods</a></li>
<li class="chapter" data-level="28.2" data-path="28-experimental-design.html"><a href="28-experimental-design.html#additional-software"><i class="fa fa-check"></i><b>28.2</b> 4.3.3 Additional software</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="29-references-3.html"><a href="29-references-3.html"><i class="fa fa-check"></i><b>29</b> References</a></li>
<li class="chapter" data-level="30" data-path="30-results-and-discussion.html"><a href="30-results-and-discussion.html"><i class="fa fa-check"></i><b>30</b> Results and discussion</a></li>
<li class="chapter" data-level="31" data-path="31-clustering.html"><a href="31-clustering.html"><i class="fa fa-check"></i><b>31</b> 5.1 Clustering</a></li>
<li class="chapter" data-level="32" data-path="32-model-building.html"><a href="32-model-building.html"><i class="fa fa-check"></i><b>32</b> 5.2 Model building</a></li>
<li class="chapter" data-level="33" data-path="33-forecasting-1.html"><a href="33-forecasting-1.html"><i class="fa fa-check"></i><b>33</b> 5.3 Forecasting</a></li>
<li class="chapter" data-level="34" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html"><i class="fa fa-check"></i><b>34</b> 5.4 Limitations &amp; Recommendations</a><ul>
<li class="chapter" data-level="34.1" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#limits-of-forecastability"><i class="fa fa-check"></i><b>34.1</b> 5.4.1 Limits of forecastability</a></li>
<li class="chapter" data-level="34.2" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#exogeneous-variables"><i class="fa fa-check"></i><b>34.2</b> 5.4.2 Exogeneous variables</a></li>
<li class="chapter" data-level="34.3" data-path="34-limitations-recommendations.html"><a href="34-limitations-recommendations.html#non-normal-distributions"><i class="fa fa-check"></i><b>34.3</b> 5.4.3 Non-normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="35-references-4.html"><a href="35-references-4.html"><i class="fa fa-check"></i><b>35</b> References</a></li>
<li class="chapter" data-level="36" data-path="36-conclusion.html"><a href="36-conclusion.html"><i class="fa fa-check"></i><b>36</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references-5.html"><a href="references-5.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Spatio-Temporal Forecasts for Bike Availability in Dockless Bike Sharing Systems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="time-series-clustering" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> 2.5 Time series clustering</h1>
<div id="dissimilarity-measures" class="section level2">
<h2><span class="header-section-number">13.1</span> 2.5.1 Dissimilarity measures</h2>
<p>Time series clustering is a specific domain within the field of time series analysis. Given a set of individual time series, its objective is to group similar time series into the same cluster <span class="citation">(Keogh &amp; Lin, 2005)</span>. Logically, this involves the calculation of a measure that represents the similarity, or dissimilarity, between two time series. A wide range of such measures exist, some of them based directly on the observations in the time series, others on specific features of the time series, and others on parameters or residual characteristics of models fitted to the time series. Whenever the time series are of the same length, and observed at the same timestamps, a simple dissimilarity measure can be calculated by summing the ordered point-to-point distances between them. The most used distance function, in this sense, is the Euclidean distance, as defined in Equation 2.x <span class="citation">(Cassisi, Montalto, Aliotta, Cannata, &amp; Pulvirenti, 2012)</span>.</p>
<p><span class="math display">\[ d(Y,X) = \sqrt{\sum_{t=1}^{n}(y_{t}-x_{t})^{2}} \]</span></p>
<p>Where <span class="math inline">\(d(X,Y)\)</span> is the dissimilarity value between time series <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math inline">\(y_{t}\)</span> and <span class="math inline">\(x_{t}\)</span> are the observations in respectively <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(n\)</span> is the length of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. A higher value of <span class="math inline">\(d(X,Y)\)</span> implies less similar time series. By definition, <span class="math inline">\(d(X,Y) \geq 0\)</span>.</p>
<p>Using Euclidean distance as a dissimilarity measure is simple, but has drawbacks for certain applications. For example, it can not handle time series of different length, it is sensitive to outliers, and it can not capture out-of-phase similarities, which occur when two time series show similar patterns, but shifted over time. To deal with one or more of these drawbacks, several other dissimilarity measures for time series were developed, of which the <em>dynamic time warping distance</em> is best known. Dynamic time warping is based on classical algorithms for comparing discrete sequences with continuous sequences, and basically replaces the one-to-one comparison of Euclidean distances with a many-to-one comparison. However, despite its simplicity, the Euclidean distance approach still turns out to be very competitive with the more complex methods <span class="citation">(Cassisi et al., 2012)</span>.</p>
<p>Once the dissimilarity values for all possible combinations between the analyzed time series are calculated, they are stored in an <span class="math inline">\(n \times n\)</span> matrix, where <span class="math inline">\(n\)</span> is the number of analyzed time series, and the diagonal entries equal zero. Such a matrix is commonly referred to as a dissimilarity matrix, and can be used to cluster the time series with conventional clustering algorithms. In time series analysis, k-means clustering and hierarchical clustering are the most popular options for this task <span class="citation">(X. C. Wang, Smith, &amp; Hyndman, 2006)</span>. The latter is used in this thesis, and its theory is summarized briefly in the next sub-section, based on <span class="citation">G. Gan, Ma, &amp; Wu (2007)</span>, Chapter 7 and Chapter 17.</p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">13.2</span> 2.5.2 Hierarchical clustering</h2>
<p>Hierarchical clustering algorithms divide a set of data points into a sequence of nested partitions, where each partition consists of a different number of clusters. Two main types are distinguished: <em>agglomerative</em> hierarchical clustering, and <em>divisive</em> hierarchical clustering. In the first, the starting point is a partition in which all data points form a cluster on their own. Then, the two closest clusters, based on a pre-defined dissimilarity measure, are merged. This process is repeated until all data points are in one single cluster. The divisive method works the other way around: all data points start in the same cluster, which is split repeatedly, until the number of clusters equals the number of data points. Agglomerative hierarchical clustering is the most popular of the two types, and forms the focus of this section.</p>
<p>Again, defining a suitable dissimilarity measure is a core task in the process. The simplest way of doing so, is known as the <em>single link method</em>, where the dissimilarity value between two clusters is defined as the shortest possible distance from a data point in the first cluster, to a data point in the second cluster. In contradiction, the <em>complete link method</em> calculates the dissimilarity value as being the longest possible distance from a data point in the first cluster, to a data point in the second cluster. Other methods include the <em>group average method</em>, which calculates the average of the shortest distances between all possible pairs of data points, and the <em>centroid method</em>, which calculates the shortest distance between the cluster centroids. In all cases, the Euclidean distance is commonly used as the distance function.</p>
<p>A more general approach is known as the <em>Ward method</em>, developed by <span class="citation">Ward Jr. (1963)</span>. In this method, the dissimilarity between two clusters is defined as the loss of information when the clusters are merged. The information loss of a merge between two clusters is quantified with Equation 2.x.</p>
<p><span class="math display">\[ \Delta I = I(C_{m}) - I(C_{1}) - I(C_{2}) \]</span></p>
<p>Where <span class="math inline">\(\Delta I\)</span> is the information loss, <span class="math inline">\(C_{m}\)</span> is the merge between clusters <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>, and <span class="math inline">\(I(C_{m})\)</span>, <span class="math inline">\(I(C_{1})\)</span> and <span class="math inline">\(I(C_{2})\)</span> are the information criteria of respectively <span class="math inline">\(C_{m}\)</span>, <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>. <span class="citation">Ward Jr. (1963)</span> did not put a hard restriction on how such an information criterion should be quantified, but usually, it is set to be the <em>error sum of squares</em> of the cluster, calculated with Equation 2.x.</p>
<p><span class="math display">\[ I(C) = \sum_{i = 1}^{n} (c_{i} - \mu(C))^{2} \]</span></p>
<p>Where <span class="math inline">\(c_{i}\)</span> are the data points in <span class="math inline">\(C\)</span>, <span class="math inline">\(\mu(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, and <span class="math inline">\(n\)</span> is the number of data points in <span class="math inline">\(C\)</span>. In other words, <span class="math inline">\(I(C)\)</span> is the sum of the Euclidean distances from all data points in the cluster, to the center of mass of the cluster.</p>
<p>Since a hierarchical clustering algorithm produces a sequence of partitions, it is not needed to define the desired number of clusters, <span class="math inline">\(k\)</span>, in advance. However, if one is interested in obtaining only one single partition, it is necessary to find a suitable value of <span class="math inline">\(k\)</span>. It is common practice to do so by visually interpreting the dendrogram of the hierarchical clustering, which is a diagram representing the output in a tree structure, but automated approaches have been developed as well. Most of them are based on the idea that in an ideal situation, clusters are compact and clearly separated from each other. However, minimizing the variance within the clusters will always favor the situation where each data point forms a cluster on its own, while maximizing the variance between the clusters will always favor the situation where all data points are together in one cluster. Therefore, most approaches combine those two operations, in order to find the best possible partition.</p>
<p>An example of such an approach is the <em>Dunn Index</em>, developed by <span class="citation">Dunn (1974)</span>. For a specific partition into <span class="math inline">\(k\)</span> clusters, it calculates the ratio of the smallest distance between two data points that are not in the same cluster to the largest distance between two data points that are in the same cluster. This is shown in Equation 2.x.</p>
<p><span class="math display">\[ V(\Lambda_{k}) = \min \Bigg\{ \min \Bigg(\frac{D(C_{i}, C_{j})}{\max diam(C_{l})} \Bigg) \Bigg\} \]</span></p>
<p>Where <span class="math inline">\(V(\Lambda_{k})\)</span> is the Dunn Index of a partition <span class="math inline">\(\Lambda_{k}\)</span> with <span class="math inline">\(k\)</span> clusters, <span class="math inline">\(1 \leq i, j, l \leq k\)</span>, <span class="math inline">\(D(C_{i}, C_{j})\)</span> is the Euclidean distance between a data point in cluster <span class="math inline">\(C_{i} \in \Lambda_{k}\)</span> and a data point in cluster <span class="math inline">\(C_{j} \in \Lambda_{k}\)</span>, given that <span class="math inline">\(C_{i} \neq C_{j}\)</span>, and <span class="math inline">\(diam(C_{l})\)</span> is the largest Euclidean distance between two data points in cluster <span class="math inline">\(C_{l} \in \Lambda_{k}\)</span>. To find the optimal partition <span class="math inline">\(\Lambda_{k}^*\)</span>, the Dunn Index should be maximized.</p>
</div>
<div id="spatial-time-series-clustering" class="section level2">
<h2><span class="header-section-number">13.3</span> 2.5.3 Spatial time series clustering</h2>
<p>Spatial time series are time series with a spatial reference, i.e. time series that are linked to geographical locations <span class="citation">(Kamarianakis &amp; Prastacos, 2005)</span>. When given a set of spatial time series, it may be desired that the clusters are not only similar in data values, but also form spatially connected sets. In that case, constraints need to be imposed on the possible outcomes of the clustering process. This can can be done in a strict way, where the resulting partition is forced to consist of spatially contiguous clusters. However, this may group very similar time series into different clusters, if they are spatially apart. Hence, an adequate balance between the data similarity and the spatial similarity, needs to be found, without artificially forcing an overestimated spatial dependence on the time series. For this, <span class="citation">Chavent, Kuentz-Simonet, Labenne, &amp; Saracco (2018)</span> developed a variation on the hierarchical clustering algorithm, called <em>spatially constrained hierarchical clustering</em>, which is summarized in this sub-section. The algorithm takes two dissimilarity matrices as input. The first one gives the dissimilarity values in the <em>feature space</em>, i.e. the dissimilarities of the data values of the observations, while the latter gives the dissimilarity values in the <em>constraint space</em>, i.e. the dissimilarities of the geographical locations of the observations.</p>
<p>Spatially constrained hierarchical clustering uses a Ward-like method to define which clusters will be merged at each step, but with a different definition of the information criterion of a cluster, as shown in Equation 2.x.</p>
<p><span class="math display">\[ 
I(C) = 
(1-\alpha)\sum_{i=1}^{n}(c_{i} - \mu(C))^{2} +
\alpha\sum_{i=1}^{n}(c_{i}^{*} - \mu^{*}(C))^{2}
\]</span></p>
<p>Where <span class="math inline">\(c_{i}\)</span> are the data points in <span class="math inline">\(C\)</span>, with normalized values taken from the feature space dissimilarity matrix, and <span class="math inline">\(\mu(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, computed with those values. <span class="math inline">\(c_{i}^{*}\)</span> are the same data points, but with normalized values taken from the constraint space dissimilarity matrix, and <span class="math inline">\(\mu^{*}(C)\)</span> is the center of mass of <span class="math inline">\(C\)</span>, computed with those values. Furthermore, <span class="math inline">\(n\)</span> is the number of data points in <span class="math inline">\(C\)</span> and <span class="math inline">\(\alpha\)</span> is the mixing parameter, where <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. <span class="citation">Chavent et al. (2018)</span> also present more general approaches, in which the calculated distances in Equation 2.x are not necessarily Euclidean, and where observations can be weighted, but these are not covered in this thesis.</p>
<p>With the information criterion of a single cluster calculated as in Equation 2.x, the information loss of a merge between two clusters is calculated in the same way as in regular Ward hierarchical clustering (Equation 2.x). Then, at each merging step, two clusters are merged such that the loss of information is minimized.</p>
<p>The mixing parameter <span class="math inline">\(\alpha\)</span> plays a key role in spatially constrained hierarchical clustering. It sets the importance that is given to the spatial constraint. The higher <span class="math inline">\(\alpha\)</span>, the more the result of the clustering procedure is influenced by the spatial locations of the data points. When <span class="math inline">\(\alpha = 0\)</span>, the data are clustered without any spatial constraint, while <span class="math inline">\(\alpha = 1\)</span> leads to a clustering only based on the spatial location of the data points. Therefore, it is important to determine a suitable value for <span class="math inline">\(\alpha\)</span>. <span class="citation">Chavent et al. (2018)</span> propose the following approach. First, <span class="math inline">\(alpha\)</span> is set to zero, and a hierarchical clustering without spatial constraint is performed. The resulting sequence of partitions is rendered as a dendrogram, and the optimal number of clusters <span class="math inline">\(k^{*}\)</span> is defined visually. Then, several spatially constrained hierarchical clustering procedures are performed, each with a different value of <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(k = k^{*}\)</span>. Since <span class="math inline">\(k\)</span> is fixed, the outputs are always single partitions. For each cluster in such a partition the information criterion regarding the feature data (i.e. the first part of Equation 2.x) and the information criterion regarding the constraint data (i.e. the second part of Equation 2.x), are calculated separately, and also summed separately over all clusters in the partition. These two summed values are then plotted, with <span class="math inline">\(\alpha\)</span> on the x-axis. In the end, this lead to a plot that shows the loss of information in the feature space and the gain of information in the constraint space, as <span class="math inline">\(\alpha\)</span> gets larger. With such a plot, <span class="math inline">\(\alpha\)</span> can be chosen such that the trade-off between the loss and gain of information in the two spaces is considered acceptable.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-time-series-forecasting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-references-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
